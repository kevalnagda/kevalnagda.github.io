<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-12-02T18:53:21+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Home</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>Keval Nagda</name></author><entry><title type="html">Machine Learning Evaluation Metrics</title><link href="http://localhost:4000/evaluation-metrics" rel="alternate" type="text/html" title="Machine Learning Evaluation Metrics" /><published>2020-12-02T05:30:00+05:30</published><updated>2020-12-02T05:30:00+05:30</updated><id>http://localhost:4000/evaluation-metrics</id><content type="html" xml:base="http://localhost:4000/evaluation-metrics">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Evaluating your machine learning model is a crucial part of any project. Your model may give satisfactory results when evaluated using metrics such as accuracy but may perform poorly when evaluated against other metrics such as loss or F1 score.&lt;/p&gt;

&lt;p&gt;In most cases, we use accuracy to measure the model performance, however, it is not enough to truly judge our model. Thus, let’s take a look at different evaluation metrics available.&lt;/p&gt;

&lt;h2 id=&quot;confusion-matrix&quot;&gt;Confusion Matrix&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Confusion Matrix&lt;/strong&gt; is a performance measurement for a machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/metrics/1.png&quot; alt=&quot;Confusion Matrix&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is very useful for measuring other evaluation metrics such as Recall, Precision, Specificity, Accuracy, and most importantly AUC-ROC Curve.&lt;/p&gt;

&lt;p&gt;Following is an example in terms of pregnancy analogy to help you better understand TP, TN, FP, and FN.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/metrics/2.png&quot; alt=&quot;Example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;True Positive&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Interpretation: You predicted positive and it’s true.&lt;/p&gt;

&lt;p&gt;You predicted that a woman is pregnant and she actually is.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;True Negative&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Interpretation: You predicted negative and it’s true.&lt;/p&gt;

&lt;p&gt;You predicted that a man is not pregnant and he actually is not.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;False Positive (Type 1 Error)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Interpretation: You predicted positive and it’s false.&lt;/p&gt;

&lt;p&gt;You predicted that a man is pregnant but he actually is not.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;False Negative (Type 2 Error)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Interpretation: You predicted negative and it’s false.&lt;/p&gt;

&lt;p&gt;You predicted that a woman is not pregnant but she actually is.&lt;/p&gt;

&lt;h2 id=&quot;positive-predictive-value-ppv-or-precision&quot;&gt;Positive Predictive Value (PPV) or Precision&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt; is the number of correct positive results divided by the number of positive results predicted by the classifier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/metrics/3.png&quot; alt=&quot;Precision&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sensitivity-or-recall&quot;&gt;Sensitivity or Recall&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt; or &lt;strong&gt;Recall&lt;/strong&gt; is another important metric, which is defined as the fraction of samples from a class that is correctly predicted by the model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/metrics/4.png&quot; alt=&quot;Recall&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;accuracy&quot;&gt;Accuracy&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt; is perhaps the simplest metrics one can imagine and is defined as the number of correct predictions divided by the total number of predictions, multiplied by 100.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/metrics/5.png&quot; alt=&quot;Accuracy&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;specificity-or-true-negative-rate&quot;&gt;Specificity or True Negative Rate&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Specificity&lt;/strong&gt; is the true negative rate or the proportion of true negatives to everything that should have been classified as negative.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/metrics/6.jpg&quot; alt=&quot;Specificity&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dice-coefficient-f1-score&quot;&gt;Dice Coefficient (F1 Score)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Dice Coefficient&lt;/strong&gt; or &lt;strong&gt;F1 Score&lt;/strong&gt; is the Harmonic Mean between precision and recall. The range for the F1 Score is [0, 1].&lt;/p&gt;

&lt;p&gt;It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/metrics/6.png&quot; alt=&quot;F1 Score&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;average-precision-ap&quot;&gt;Average Precision (AP)&lt;/h2&gt;

&lt;p&gt;Although the precision-recall curve can be used to evaluate the performance of a detector, it is not easy to compare among different detectors when the curves intersect with each other. It would be better if we have a numerical metric that can be used directly for the comparison.&lt;/p&gt;

&lt;p&gt;This is where &lt;strong&gt;Average Precision (AP)&lt;/strong&gt;, which is based on the precision-recall curve, comes into play. In essence, AP is the precision averaged across all unique recall levels.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/metrics/7.png&quot; alt=&quot;Average Precision&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where, &lt;em&gt;r&lt;sub&gt;1&lt;/sub&gt;, r&lt;sub&gt;2&lt;/sub&gt;, r&lt;sub&gt;3&lt;/sub&gt;, …, r&lt;sub&gt;n&lt;/sub&gt;&lt;/em&gt; are the recall levels at which the precision is first interpolated.&lt;/p&gt;

&lt;h2 id=&quot;roc-curve&quot;&gt;ROC Curve&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Receiver Operating Characteristic&lt;/strong&gt; curve is a plot that shows the performance of a binary classifier as a function of its cut-off threshold.&lt;/p&gt;

&lt;p&gt;It essentially shows the True Positive Rate (TPR) against the False Positive Rate (FPR) for various threshold values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/metrics/8.png&quot; alt=&quot;ROC Curve&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;auc&quot;&gt;AUC&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Area Under the Curve (AUC)&lt;/strong&gt;, is an aggregated measure of performance of a binary classifier on all possible threshold values (and therefore it is threshold invariant).&lt;/p&gt;

&lt;p&gt;AUC calculates the area under the ROC curve, and therefore it is between 0 and 1. One way of interpreting AUC is the probability that the model ranks a random positive example more highly than a random negative example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/metrics/9.png&quot; alt=&quot;AUC&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;youdens-index&quot;&gt;Youden’s index&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Youden’s &lt;em&gt;J&lt;/em&gt; statistic&lt;/strong&gt; (also called &lt;strong&gt;Youden’s index&lt;/strong&gt;) is a single statistic that captures the performance of a dichotomous (A partition of a whole into two) diagnostic tests.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Youden’s &lt;em&gt;J&lt;/em&gt; statistic&lt;/strong&gt; is&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;J = sensitivity + specificity - 1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The right-hand two quantities are sensitivity and specificity and thus, the expanded formula is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/metrics/10.png&quot; alt=&quot;Youden&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Youden’s index is often used in conjunction with &lt;strong&gt;receiver operating characteristic (ROC)&lt;/strong&gt; analysis. The index is defined for all points of a ROC curve, and the maximum value of the index may be used as a criterion for selecting the optimum cut-off point when a diagnostic test gives a numeric rather than a dichotomous result.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62&quot;&gt;Understanding Confusion Matrix&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&quot;&gt;Youden’s J Statistic&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Keval Nagda</name></author><category term="jekyll" /><category term="update" /><summary type="html">Introduction Evaluating your machine learning model is a crucial part of any project. Your model may give satisfactory results when evaluated using metrics such as accuracy but may perform poorly when evaluated against other metrics such as loss or F1 score. In most cases, we use accuracy to measure the model performance, however, it is not enough to truly judge our model. Thus, let’s take a look at different evaluation metrics available. Confusion Matrix Confusion Matrix is a performance measurement for a machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values as shown below. It is very useful for measuring other evaluation metrics such as Recall, Precision, Specificity, Accuracy, and most importantly AUC-ROC Curve. Following is an example in terms of pregnancy analogy to help you better understand TP, TN, FP, and FN. True Positive Interpretation: You predicted positive and it’s true. You predicted that a woman is pregnant and she actually is. True Negative Interpretation: You predicted negative and it’s true. You predicted that a man is not pregnant and he actually is not. False Positive (Type 1 Error) Interpretation: You predicted positive and it’s false. You predicted that a man is pregnant but he actually is not. False Negative (Type 2 Error) Interpretation: You predicted negative and it’s false. You predicted that a woman is not pregnant but she actually is. Positive Predictive Value (PPV) or Precision Precision is the number of correct positive results divided by the number of positive results predicted by the classifier. Sensitivity or Recall Sensitivity or Recall is another important metric, which is defined as the fraction of samples from a class that is correctly predicted by the model. Accuracy Accuracy is perhaps the simplest metrics one can imagine and is defined as the number of correct predictions divided by the total number of predictions, multiplied by 100. Specificity or True Negative Rate Specificity is the true negative rate or the proportion of true negatives to everything that should have been classified as negative. Dice Coefficient (F1 Score) Dice Coefficient or F1 Score is the Harmonic Mean between precision and recall. The range for the F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances). Average Precision (AP) Although the precision-recall curve can be used to evaluate the performance of a detector, it is not easy to compare among different detectors when the curves intersect with each other. It would be better if we have a numerical metric that can be used directly for the comparison. This is where Average Precision (AP), which is based on the precision-recall curve, comes into play. In essence, AP is the precision averaged across all unique recall levels. where, r1, r2, r3, …, rn are the recall levels at which the precision is first interpolated. ROC Curve The Receiver Operating Characteristic curve is a plot that shows the performance of a binary classifier as a function of its cut-off threshold. It essentially shows the True Positive Rate (TPR) against the False Positive Rate (FPR) for various threshold values. AUC The Area Under the Curve (AUC), is an aggregated measure of performance of a binary classifier on all possible threshold values (and therefore it is threshold invariant). AUC calculates the area under the ROC curve, and therefore it is between 0 and 1. One way of interpreting AUC is the probability that the model ranks a random positive example more highly than a random negative example. Youden’s index Youden’s J statistic (also called Youden’s index) is a single statistic that captures the performance of a dichotomous (A partition of a whole into two) diagnostic tests. Youden’s J statistic is J = sensitivity + specificity - 1 The right-hand two quantities are sensitivity and specificity and thus, the expanded formula is as follows: Youden’s index is often used in conjunction with receiver operating characteristic (ROC) analysis. The index is defined for all points of a ROC curve, and the maximum value of the index may be used as a criterion for selecting the optimum cut-off point when a diagnostic test gives a numeric rather than a dichotomous result. Reference Understanding Confusion Matrix Youden’s J Statistic</summary></entry><entry><title type="html">Transfer Learning using PyTorch</title><link href="http://localhost:4000/transfer-learning" rel="alternate" type="text/html" title="Transfer Learning using PyTorch" /><published>2020-11-26T05:30:00+05:30</published><updated>2020-11-26T05:30:00+05:30</updated><id>http://localhost:4000/transfer-learning</id><content type="html" xml:base="http://localhost:4000/transfer-learning">&lt;p&gt;Today we learn how to perform transfer learning for image classification using PyTorch.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;h2 id=&quot;what-is-pytorch&quot;&gt;What is PyTorch?&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt; is an open-source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook’s AI Research lab (FAIR).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;what-is-transfer-learning&quot;&gt;What is Transfer Learning?&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Transfer learning (TL)&lt;/strong&gt; is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.&lt;/p&gt;

  &lt;p&gt;For example, the knowledge gained while learning to recognize cars could apply when trying to recognize trucks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;getting-started&quot;&gt;Getting started&lt;/h1&gt;

&lt;p&gt;We use a Kaggle Notebook for this task since it provides free computation services which should be sufficient for the image classification task.&lt;/p&gt;

&lt;h2 id=&quot;import-libraries&quot;&gt;Import Libraries&lt;/h2&gt;

&lt;p&gt;We import various libraries along with the submodules required for the image classification.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%matplotlib inline
%reload_ext autoreload
%autoreload 2
 
# Import required libraries
import numpy as np
import pandas as pd
import time
import math
import tqdm as tqdm
import os
import csv
import torch
import torch.nn.functional as F
import torch.optim as optim
import torchvision.models as models
import matplotlib.pyplot as plt
from PIL import Image
from torch import nn
from torch.utils.data.dataset import Dataset
from torch.utils.data import DataLoader
from torchvision import transforms, datasets
from torch.autograd import Variable
 
# Check if compute other than CPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;load-data&quot;&gt;Load Data&lt;/h2&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;We use the &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html&quot;&gt;102 Category Flower Dataset&lt;/a&gt; for our image classification task. This dataset contains images of flowers belonging to 102 different categories.&lt;/p&gt;

&lt;h3 id=&quot;set-data-folders&quot;&gt;Set data folders&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;data_dir = '../input/oxford-flowers-data/flower_data'
train_dir = data_dir + '/train'
valid_dir = data_dir + '/valid'
test_dir = data_dir + '/test'
name_json = data_dir + '/cat_to_name.json'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we have three main directories &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_dir&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;valid_dir&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test_dir&lt;/code&gt; which contain training, validation, and testing data respectively.&lt;/p&gt;

&lt;p&gt;We also have a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat_to_name.json&lt;/code&gt; file which contains the mapping of flower category to the name of the flower.&lt;/p&gt;

&lt;h3 id=&quot;load-test-data&quot;&gt;Load test data&lt;/h3&gt;

&lt;p&gt;The dataset at hand contains training and validation images categorized inside their respective folders, however, the test images are not categorized and are merged in a single directory.&lt;/p&gt;

&lt;p&gt;Thus, we need to define functions to load the test data as required by PyTorch during the training of the model.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def pil_loader(path):
    with open(path, 'rb') as f:
        img = Image.open(f)
        return img.convert('RGB')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class TestDataset(torch.utils.data.Dataset):
    def __init__(self, path, transform=None):
        self.path = path
        self.files = []
        for (dirpath, _, filenames) in os.walk(self.path):
            for f in filenames:
                if f.endswith('.jpg'):
                    p = {}
                    p['img_path'] = dirpath + '/' + f
                    self.files.append(p)
        self.transform = transform
 
    def __len__(self):
        return len(self.files)
 
    def __getitem__(self, idx):
        img_path = self.files[idx]['img_path']
        img_name = img_path.split('/')[-1]
        image = pil_loader(img_path)
        if self.transform:
            image = self.transform(image)
        return image, 0, img_name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;label-mapping&quot;&gt;Label mapping&lt;/h3&gt;

&lt;p&gt;We load the labels JSON file and define a function to get the class/category of a flower by passing in an index.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import json
 
with open(name_json, 'r') as f:
    cat_to_name = json.load(f)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def get_cat_name(index):
    return cat_to_name[idx_to_class[index]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;transform&quot;&gt;Transform&lt;/h3&gt;

&lt;p&gt;Now even though we might have enough data for training but performing various transforms or operations on the images helps the model learn better and perform efficiently on unseen data.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]
 
normalize = transforms.Normalize(mean=mean, std=std)
data_transforms = transforms.Compose([
                    transforms.Pad(4, padding_mode='reflect'),
                    transforms.RandomRotation(10),
                    transforms.RandomResizedCrop(224),
                    transforms.RandomHorizontalFlip(),
                    transforms.ToTensor(),
                    normalize
                ])
test_transforms = transforms.Compose([
                    #transforms.Pad(4, padding_mode='reflect'),
                    transforms.RandomResizedCrop(224),
                    transforms.ToTensor(),
                    normalize
                ])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;dataloader&quot;&gt;Dataloader&lt;/h3&gt;

&lt;p&gt;Next, we provide the image dataset directories and the respective transforms to the dataloader function which will actually load images as needed.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_datasets = datasets.ImageFolder(train_dir, data_transforms)
val_datasets = datasets.ImageFolder(valid_dir, test_transforms)
test_datasets = TestDataset(test_dir, test_transforms)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bs = 64
 
trainloader = torch.utils.data.DataLoader(train_datasets, batch_size=bs, shuffle=True)
validloader = torch.utils.data.DataLoader(val_datasets, batch_size=bs, shuffle=True)
testloader = torch.utils.data.DataLoader(test_datasets, batch_size=1, shuffle=False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, we create a dictionary to get the class/category of a flower from an index.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;idx_to_class = {val:key for key, val in val_datasets.class_to_idx.items()}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;plot-images&quot;&gt;Plot images&lt;/h3&gt;

&lt;p&gt;Its always a good idea to take a look at the dataset before training the model. Hence, we define a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;plot_img&lt;/code&gt; function to plot a grid of random dataset images.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def plot_img(preds=None, is_pred=False):        
    fig = plt.figure(figsize=(8,8))
    columns = 4
    rows = 5
 
    for i in range(1, columns*rows +1):
        fig.add_subplot(rows, columns, i)
        
        if is_pred:
            img_xy = np.random.randint(len(test_datasets));
            img = test_datasets[img_xy][0].numpy()           
        else:
            img_xy = np.random.randint(len(train_datasets));
            img = train_datasets[img_xy][0].numpy()
            
        img = img.transpose((1, 2, 0))
        img = std * img + mean
        
        if is_pred:
            plt.title(get_cat_name(preds[img_xy]) + &quot;/&quot; + get_cat_name(test_datasets[img_xy][1]))
        else:
            plt.title(str(get_cat_name(train_datasets[img_xy][1])))
        plt.axis('off')
        img = np.clip(img, 0, 1)
        plt.imshow(img, interpolation='nearest')
    plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plot_img()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/transfer_learning/1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;helper-functions&quot;&gt;Helper functions&lt;/h2&gt;

&lt;p&gt;Next, we define helper functions which will be required later while training and evaluating the model.&lt;/p&gt;

&lt;p&gt;A function that returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accuracy&lt;/code&gt; based on the output and target parameters.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def accuracy(output, target, is_test=False):
    global total
    global correct
    batch_size = target.size(0)
    total += batch_size    
    _, pred = output.max(dim=1)
    if is_test:
        preds.extend(pred)
    correct += torch.sum(pred == target.data)
    return  (correct.float()/total) * 100
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reset&lt;/code&gt; function to reset/clear evaluation metrics.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def reset():
    global total, correct
    global train_loss, test_loss, best_acc
    global trn_losses, trn_accs, val_losses, val_accs
    total, correct = 0, 0
    train_loss, test_loss, best_acc = 0.0, 0.0, 0.0
    trn_losses, trn_accs, val_losses, val_accs = [], [], [], []
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A function to record the training and testing stats used to plot graphs.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class AvgStats(object):
    def __init__(self):
        self.reset()
        
    def reset(self):
        self.losses =[]
        self.precs =[]
        self.its = []
        
    def append(self, loss, prec, it):
        self.losses.append(loss)
        self.precs.append(prec)
        self.its.append(it)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A function to save checkpoint if a new best score is achieved while training&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def save_checkpoint(model, is_best, filename='./checkpoint.pth.tar'):
    if is_best:
        torch.save(model.state_dict(), filename)  # save checkpoint
    else:
        print (&quot;=&amp;gt; Validation Accuracy did not improve&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A function to load a saved checkpoint.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def load_checkpoint(model, filename = './checkpoint.pth.tar'):
    sd = torch.load(filename, map_location=lambda storage, loc: storage)
    names = set(model.state_dict().keys())
    for n in list(sd.keys()): 
        if n not in names and n+'_raw' in names:
            if n+'_raw' not in sd: sd[n+'_raw'] = sd[n]
            del sd[n]
    model.load_state_dict(sd)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;cyclic-learning-rate&quot;&gt;Cyclic Learning Rate&lt;/h2&gt;

&lt;p&gt;This method is described in the paper &lt;a href=&quot;https://arxiv.org/abs/1506.01186&quot;&gt;Cyclical Learning Rates for Training Neural Networks&lt;/a&gt; to find out the optimum learning rate.&lt;/p&gt;

&lt;p&gt;The learning rate is increased from a lower value to a higher per iteration for some iterations till loss starts exploding. The learning rate one power lower than the one where loss is minimum is chosen as the optimum learning rate for training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class CLR(object):
    def __init__(self, optim, bn, base_lr=1e-7, max_lr=100):
        self.base_lr = base_lr
        self.max_lr = max_lr
        self.optim = optim
        self.bn = bn - 1
        ratio = self.max_lr/self.base_lr
        self.mult = ratio ** (1/self.bn)
        self.best_loss = 1e9
        self.iteration = 0
        self.lrs = []
        self.losses = []
        
    def calc_lr(self, loss):
        self.iteration +=1
        if math.isnan(loss) or loss &amp;gt; 4 * self.best_loss:
            return -1
        if loss &amp;lt; self.best_loss and self.iteration &amp;gt; 1:
            self.best_loss = loss
            
        mult = self.mult ** self.iteration
        lr = self.base_lr * mult
        
        self.lrs.append(lr)
        self.losses.append(loss)
        
        return lr
        
    def plot(self, start=10, end=-5):
        plt.xlabel(&quot;Learning Rate&quot;)
        plt.ylabel(&quot;Losses&quot;)
        plt.plot(self.lrs[start:end], self.losses[start:end])
        plt.xscale('log')
        
        
    def plot_lr(self):
        plt.xlabel(&quot;Iterations&quot;)
        plt.ylabel(&quot;Learning Rate&quot;)
        plt.plot(self.lrs)
        plt.yscale('log')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;lookahead&quot;&gt;Lookahead&lt;/h2&gt;

&lt;p&gt;Here we implement Lookahead optimizer. As proposed, the Lookahead optimizer goes k steps forward and 1 step backward.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torch.optim import Optimizer
from collections import defaultdict
 
 
class Lookahead(Optimizer):
    def __init__(self, optimizer, alpha=0.5, k=5):
        assert(0.0 &amp;lt;= alpha &amp;lt;= 1.0)
        assert(k &amp;gt;= 1)
        self.optimizer = optimizer
        self.alpha = alpha
        self.k = k
        self.param_groups = self.optimizer.param_groups
        self.state = defaultdict(dict)
        for group in self.param_groups:
            group['k_counter'] = 0
        self.slow_weights = [[param.clone().detach() for param in group['params']] for group in self.param_groups]
    
    def step(self, closure=None):
        loss = self.optimizer.step(closure)
        for group, slow_Weight in zip(self.param_groups, self.slow_weights):
            group['k_counter'] += 1
            if group['k_counter'] == self.k:
                for param, weight in zip(group['params'], slow_Weight):
                    weight.data.add_(self.alpha, (param.data - weight.data))
                    param.data.copy_(weight.data)
                group['k_counter'] = 0
 
        return loss
 
    def state_dict(self):
        fast_dict = self.optimizer.state_dict()
        fast_state = fast_dict['state']
        param_groups = fast_dict['param_groups']
        slow_state = {(id(k) if isinstance(k, torch.Tensor) else k): v
                        for k, v in self.state.items()}
        return {
            'fast_state': fast_state,
            'param_groups': param_groups,
            'slow_state': slow_state
        }
 
    def load_state_dict(self, state_dict):
        fast_dict = {
            'state': state_dict['fast_state'],
            'param_groups': state_dict['param_groups']
        }
        slow_dict = {
            'state': state_dict['slow_state'],
            'param_groups': state_dict['param_groups']
        }
        super(Lookahead, self).load_state_dict(slow_dict)
        self.optimizer.load_state_dict(fast_dict)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;lr-finder&quot;&gt;LR Finder&lt;/h2&gt;

&lt;p&gt;Now, we define the logic to find Learning Rate (LR) using CLR.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def update_lr(optimizer, lr):
    for g in optimizer.param_groups:
        g['lr'] = lr
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def lr_find(clr, model, optimizer=None):
    t = tqdm.tqdm(trainloader, leave=False, total=len(trainloader))
    running_loss = 0.
    avg_beta = 0.98
    model.train()
    for i, (input, target) in enumerate(t):
        input, target = input.to(device), target.to(device)
        var_ip, var_tg = Variable(input), Variable(target)
        output = model(var_ip)
        loss = criterion(output, var_tg)
    
        running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()
        smoothed_loss = running_loss / (1 - avg_beta**(i+1))
        t.set_postfix(loss=smoothed_loss)
    
        lr = clr.calc_lr(smoothed_loss)
        if lr == -1 :
            break
        update_lr(optimizer, lr)   
    
        # compute gradient and do SGD step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;train-and-test&quot;&gt;Train and Test&lt;/h2&gt;

&lt;p&gt;We implement the train and test logic along with the fit method which will be used in the next step for model training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def train(epoch=0, model=None, optimizer=None):
    model.train()
    global best_acc
    global trn_accs, trn_losses
    is_improving = True
    counter = 0
    running_loss = 0.
    avg_beta = 0.98
    for i, (input, target) in enumerate(trainloader):
        bt_start = time.time()
        input, target = input.to(device), target.to(device)
        var_ip, var_tg = Variable(input), Variable(target)
                                    
        output = model(var_ip)
        loss = criterion(output, var_tg)
            
        running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()
        smoothed_loss = running_loss / (1 - avg_beta**(i+1))
        
        trn_losses.append(smoothed_loss)
            
        # measure accuracy and record loss
        prec = accuracy(output.data, target)
        trn_accs.append(prec)
 
        train_stats.append(smoothed_loss, prec, time.time()-bt_start)
        if prec &amp;gt; best_acc :
            best_acc = prec
            save_checkpoint(model, True)
 
        # compute gradient and do SGD step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def test(model=None):
    with torch.no_grad():
        model.eval()
        global val_accs, val_losses
        running_loss = 0.
        avg_beta = 0.98
        for i, (input, target) in enumerate(validloader):
            bt_start = time.time()
            input, target = input.to(device), target.to(device)
            var_ip, var_tg = Variable(input), Variable(target)
            output = model(var_ip)
            loss = criterion(output, var_tg)
        
            running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()
            smoothed_loss = running_loss / (1 - avg_beta**(i+1))
 
            # measure accuracy and record loss
            prec = accuracy(output.data, target, is_test=True)
            test_stats.append(loss.item(), prec, time.time()-bt_start)
        
            val_losses.append(smoothed_loss)
            val_accs.append(prec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def fit(model=None, sched=None, optimizer=None):
    print(&quot;Epoch\tTrn_loss\tVal_loss\tTrn_acc\t\tVal_acc&quot;)
    for j in range(epoch):
        train(epoch=j, model=model, optimizer=optimizer)
        test(model)
        if sched:
            sched.step(j)
        print(&quot;{}\t{:06.8f}\t{:06.8f}\t{:06.8f}\t{:06.8f}&quot;
              .format(j+1, trn_losses[-1], val_losses[-1], trn_accs[-1], val_accs[-1]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;model-and-training&quot;&gt;Model and Training&lt;/h2&gt;

&lt;h3 id=&quot;initialize-variables&quot;&gt;Initialize variables&lt;/h3&gt;

&lt;p&gt;We initialize all the variables that will be required later while training and evaluating the model.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_loss = 0.0
test_loss = 0.0
best_acc = 0.0
trn_losses = []
trn_accs = []
val_losses = []
val_accs = []
total = 0
correct = 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;We use a pretrained ResNet50, a variant of the ResNet model which has 48 Convolution layers along with 1 MaxPool and 1 Average Pool layer.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = models.resnet50(pretrained=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we configure the last fully connected layer of the model and save the model checkpoint.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model.fc = nn.Linear(in_features=model.fc.in_features, out_features=102)
 
for param in model.parameters():
    param.require_grad = False
    
for param in model.fc.parameters():
    param.require_grad = True
    
model = model.to(device)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;save_checkpoint(model, True, 'before_start_resnet50.pth.tar')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Define loss and optimizer, find learning rate, and plot a graph.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;criterion = nn.CrossEntropyLoss()
optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)
optimizer = Lookahead(optim)
 
clr = CLR(optim, len(trainloader))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lr_find(clr, model, optim)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;clr.plot()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/transfer_learning/2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-before-unfreezing&quot;&gt;Training before unfreezing&lt;/h3&gt;

&lt;p&gt;Let’s load back the ResNet checkpoint that we saved earlier.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;load_checkpoint(model, 'before_start_resnet50.pth.tar')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Initialize parameters for the model training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;preds = []
epoch = 30
train_stats = AvgStats()
test_stats = AvgStats()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We reset the parameters to makes sure that we record fresh results while training the model.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;reset()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Define loss and optimizer just like before.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;criterion = nn.CrossEntropyLoss()
optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)
optimizer = Lookahead(optim)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, train the model for 30 epochs.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fit(model=model, optimizer=optimizer)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Save the model checkpoint after training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;save_checkpoint(model, True, 'before_unfreeze_resnet50.pth.tar')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We plot training and validation loss as well as training and validation accuracy graphs to check the model performance.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plt.xlabel(&quot;Iterations&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.plot(train_stats.precs, 'r', label='Train')
plt.legend()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/transfer_learning/3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plt.xlabel(&quot;Iterations&quot;)
plt.ylabel(&quot;Loss&quot;)
plt.plot(train_stats.losses, 'r', label='Train')
plt.legend()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/transfer_learning/4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plt.xlabel(&quot;Iterations&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.plot(test_stats.precs, 'b', label='Valid')
plt.legend()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/transfer_learning/5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plt.xlabel(&quot;Iterations&quot;)
plt.ylabel(&quot;Loss&quot;)
plt.plot(test_stats.losses, 'b', label='Valid')
plt.legend()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/transfer_learning/6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-after-unfreezing&quot;&gt;Training after unfreezing&lt;/h3&gt;

&lt;p&gt;Initialize parameters for the model training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;preds = []
train_stats = AvgStats()
test_stats = AvgStats()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We reset the parameters to makes sure that we record fresh results while training the model.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;reset()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Enable differentiation tracking for each model parameter.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for param in model.parameters():
    param.require_grad = True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Define the updated loss and optimizer and plot a losses vs learning rate graph.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;optim = torch.optim.SGD(model.parameters(), lr=1e-6, momentum=0.9, weight_decay=1e-4)
clr = CLR(optim, len(trainloader), base_lr=1e-9, max_lr=10)
lr_find(clr, model, optim)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;clr.plot(start=0)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/transfer_learning/7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s load back the resnet checkpoint that we saved earlier.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;load_checkpoint(model, 'before_unfreeze_resnet50.pth.tar')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Initialize parameters for the model training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;preds = []
epoch = 10
train_stats = AvgStats()
test_stats = AvgStats()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We reset the parameters to makes sure that we record fresh results while training the model.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;reset()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Define the updated learning rate and optimizer for training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;optim = torch.optim.SGD(model.parameters(), lr=1e-9, momentum=0.9, weight_decay=1e-4)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At last, train the model for 10 epochs and save a checkpoint after training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fit(model=model, optimizer=optimizer)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;save_checkpoint(model, True, 'after_unfreeze_resnet50.pth.tar')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we plot training and validation loss to compare the performance of the model.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plt.xlabel(&quot;Iterations&quot;)
plt.ylabel(&quot;Loss&quot;)
plt.plot(train_stats.losses, 'r', label='Train')
plt.legend()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/transfer_learning/8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plt.xlabel(&quot;Iterations&quot;)
plt.ylabel(&quot;Loss&quot;)
plt.plot(test_stats.losses, 'b', label='Valid')
plt.legend()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/transfer_learning/9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;testing&quot;&gt;Testing&lt;/h2&gt;

&lt;p&gt;We define a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predict&lt;/code&gt; method to predict labels for the test dataset.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def predict(img, model):
    model.eval()
    with torch.no_grad():
        input = Variable(img)
        input = input.to(device)
        output = model(input)
        _, pred = output.max(dim=1)
        return pred[0].item()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Predict labels or classes for all the images in the test dataset.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;result = dict()
for i, (input, _, path) in enumerate(testloader):
    predicted = predict(input, model)
    result[path[0]] = idx_to_class[predicted]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Save the result for future reference.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;csv_file = open('dict.csv', 'w')
writer = csv.writer(csv_file)
writer.writerow(['file_name','id'])
for key, value in result.items():
    writer.writerow([key, value])
csv_file.close()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&quot;&gt;Deep Learning with PyTorch: A 60 Minute Blitz&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html&quot;&gt;Transfer Learning for Computer Vision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html&quot;&gt;Writing Custom Datasets, Dataloaders, and Transforms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/nachiket273/flower-classification-lookahead-radam&quot;&gt;Flower classification lookahead sgd&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Keval Nagda</name></author><category term="jekyll" /><category term="update" /><summary type="html">Today we learn how to perform transfer learning for image classification using PyTorch. Introduction</summary></entry><entry><title type="html">CT Windowing</title><link href="http://localhost:4000/ct-windowing" rel="alternate" type="text/html" title="CT Windowing" /><published>2020-11-26T05:30:00+05:30</published><updated>2020-11-26T05:30:00+05:30</updated><id>http://localhost:4000/ct-windowing</id><content type="html" xml:base="http://localhost:4000/ct-windowing">&lt;p&gt;&lt;strong&gt;Windowing&lt;/strong&gt; if simply put, is an image processing task for a CT scan that helps highlighting key anatomy so that the images can be analyzed easily.&lt;/p&gt;

&lt;p&gt;In other words, Windowing, also known as &lt;strong&gt;grey-level mapping&lt;/strong&gt;, &lt;strong&gt;contrast stretching&lt;/strong&gt;, &lt;strong&gt;histogram modification&lt;/strong&gt;, or &lt;strong&gt;contrast enhancement&lt;/strong&gt; is the process in which the CT scan greyscale component of an image is manipulated via the CT numbers (Hounsfield Units).&lt;/p&gt;

&lt;p&gt;Changing windowing settings helps us modify/enhance the appearance of the CT image and highlight particular features in it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/1.png&quot; alt=&quot;Impact of Windowing on the appearnace of a CT scan.&quot; /&gt;&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;i&gt; Impact of Windowing on the appearnace of a CT scan. &lt;/i&gt; &lt;/div&gt;

&lt;h2 id=&quot;hounsfield-unit-hu&quot;&gt;Hounsfield Unit (HU)&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Hounsfield Units (HU)&lt;/strong&gt; are a dimensionless unit universally used in computed tomography (CT) scanning to express CT numbers in a standardized and convenient form.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Well, Hounsfield Units is related to the composition and nature of the tissue and thus, represent the density of various tissues. Note that the higher the HU value the more dense a material is and vice-versa.&lt;/p&gt;

&lt;p&gt;Below are a few values on the HU Scale that are important to keep in mind while analyzing CT images.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/2.png&quot; alt=&quot;Hounsfield Scale.&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;window-width-and-window-level&quot;&gt;Window Width and Window Level&lt;/h2&gt;

&lt;h3 id=&quot;window-width-ww&quot;&gt;Window Width (WW)&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The &lt;strong&gt;window Width (WW)&lt;/strong&gt; as the name suggests is the measure of the range of CT numbers that a CT image contains.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Window width refers to the number of different HU units that will be represented by different shades of gray.&lt;/p&gt;

&lt;p&gt;Any HU value that falls below the lower value of the window width will show up as &lt;em&gt;black&lt;/em&gt; on the scan while any HU value that is above the upper value of the window width will be &lt;em&gt;white&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;window-level-wl&quot;&gt;Window Level (WL)&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The &lt;strong&gt;Window Level (WL)&lt;/strong&gt; refers to the window centre or midpoint HU value that is represented on the window setting.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The lower the window level is set for an image, the brighter the entire image will become.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/3.png&quot; alt=&quot;Difference between window width and window level&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;windowing-settings&quot;&gt;Windowing settings&lt;/h2&gt;

&lt;p&gt;Understanding the difference between window width and window level is not enough and thus, let’s look at how changing these values can have a significant impact on image interpretation.&lt;/p&gt;

&lt;h3 id=&quot;adjusting-the-window-width&quot;&gt;Adjusting the Window Width&lt;/h3&gt;

&lt;p&gt;As the width of the window increases a larger change in density will be required to change the shade of grey that represents a certain HU unit. This results in a loss of contrast as more structures will appear similar (despite having different densities) as there is an increase in the range of CT numbers.&lt;/p&gt;

&lt;p&gt;Hence, &lt;strong&gt;increasing the window width&lt;/strong&gt; will &lt;strong&gt;decrease the contrast&lt;/strong&gt; of the image.&lt;/p&gt;

&lt;h3 id=&quot;adjusting-the-window-width-1&quot;&gt;Adjusting the Window Width&lt;/h3&gt;

&lt;p&gt;Now, as the width of the window decreases, a smaller change in density will result in a change in color on the study images. This will give increased contrast as structures that are close in density will have different shades of grey or white or black assigned to them.&lt;/p&gt;

&lt;p&gt;Hence, &lt;strong&gt;decreasing the window width&lt;/strong&gt; will &lt;strong&gt;increase the contrast&lt;/strong&gt; of the image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/4.png&quot; alt=&quot;Impact of window width on CT image&quot; /&gt;&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;i&gt; Impact of window width on the appearnace of a CT scan. &lt;/i&gt; &lt;/div&gt;

&lt;h3 id=&quot;adjusting-the-window-level&quot;&gt;Adjusting the Window Level&lt;/h3&gt;

&lt;p&gt;As the window level is increased, the image will get darker. This is because as the level is increased, higher HU values will be required in order for a density to be represented as white and many lower values will be ignored (considered as black).&lt;/p&gt;

&lt;p&gt;Hence, &lt;strong&gt;increasing the window level&lt;/strong&gt; will &lt;strong&gt;decrease the brightness&lt;/strong&gt; of the image.&lt;/p&gt;

&lt;h3 id=&quot;adjusting-the-window-level-1&quot;&gt;Adjusting the Window Level&lt;/h3&gt;

&lt;p&gt;Now, as the window level is increased, the image will get darker. This is because as the level is decreased, lower HU values will be required in order for a density to be represented as white, and thus, more white values will be allowed to pass which makes the image brighter.&lt;/p&gt;

&lt;p&gt;Hence, &lt;strong&gt;decreasing the window level&lt;/strong&gt; will &lt;strong&gt;increase the brightness&lt;/strong&gt; of the image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/5.png&quot; alt=&quot;Impact of window level on CT image&quot; /&gt;&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;i&gt; Impact of window level on the appearnace of a CT scan. &lt;/i&gt; &lt;/div&gt;

&lt;h2 id=&quot;typical-window-settings&quot;&gt;Typical window settings&lt;/h2&gt;

&lt;p&gt;Below is a list of various window settings that may somewhat vary from institution to institution. However, they are generally fairly similar.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/6.jpg&quot; alt=&quot;List of various window settings&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, the numbers shown in the figure represent two different values, the first being the window width (WW) and the second being window level (wl).&lt;/p&gt;

&lt;h3 id=&quot;bone-window&quot;&gt;Bone Window&lt;/h3&gt;

&lt;p&gt;As the name suggests, a bone window is useful for viewing the bones. A high window level near the density of bone (given its density the level is &lt;strong&gt;high&lt;/strong&gt;) is used with a wide window to give a good resolution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Window settings:&lt;/strong&gt; (W:2000, L:500)  or (W:3000, L:1000)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/7.jpg&quot; alt=&quot;Bone window&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;lung-window&quot;&gt;Lung Window&lt;/h3&gt;

&lt;p&gt;This window is used to evaluate the lungs. A high window level near the density of lung tissue (given its low density the level is &lt;strong&gt;low&lt;/strong&gt;) and is used with a wide window to give good resolution and to visualize a wide variety of densities in the chest such as the lung parenchyma as well as adjacent blood vessels.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Window settings:&lt;/strong&gt; (W:1600, L:-600) or (W:1500, L:-500)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/8.jpg&quot; alt=&quot;Lung window&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;abdomen-window&quot;&gt;Abdomen Window&lt;/h3&gt;

&lt;p&gt;The abdomen window is used to evaluate the abdominal cavity and its contents. A window level close to fluid or soft tissue is used with a medium-sized window that gives a moderate amount of contrast.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Window settings:&lt;/strong&gt; (W:400, L:40)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/9.jpg&quot; alt=&quot;Abdomen window&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;brain-window&quot;&gt;Brain Window&lt;/h3&gt;

&lt;p&gt;As the name suggests, the brain window is designed to evaluate the brain parenchyma (typically on a non-contrast scan) and is especially sensitive at picking up intracranial hemorrhage with a narrow window and a level close to the density of expected hemorrhage in the brain.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Window settings:&lt;/strong&gt; (W:70, L:30) or (W:70, L:35)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/10.jpg&quot; alt=&quot;Brain window&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;soft-tissue-window&quot;&gt;Soft Tissue Window&lt;/h3&gt;

&lt;p&gt;As the name suggests, this window is used to evaluate soft tissues. The window level is set at the density of soft tissues (50 HU) and a moderate-sized window is used to give a balance between contrast and resolution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Window settings:&lt;/strong&gt; (W:350, L:50)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/11.jpg&quot; alt=&quot;Soft Tissue window&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;liver-window&quot;&gt;Liver Window&lt;/h3&gt;

&lt;p&gt;This window is similar to the abdominal window however utilizes a narrower window to increase the contrast in the liver parenchyma (in order to make finding hepatic lesions a bit easier).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Window settings:&lt;/strong&gt; (W:160, L:60)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/windowing/12.jpg&quot; alt=&quot;Liver window&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.stepwards.com/?page_id=21646&quot;&gt;Fundamentals Of Computed Tomography Studies: Windowing&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Keval Nagda</name></author><category term="jekyll" /><category term="update" /><summary type="html">Windowing if simply put, is an image processing task for a CT scan that helps highlighting key anatomy so that the images can be analyzed easily. In other words, Windowing, also known as grey-level mapping, contrast stretching, histogram modification, or contrast enhancement is the process in which the CT scan greyscale component of an image is manipulated via the CT numbers (Hounsfield Units). Changing windowing settings helps us modify/enhance the appearance of the CT image and highlight particular features in it. Impact of Windowing on the appearnace of a CT scan. Hounsfield Unit (HU) Hounsfield Units (HU) are a dimensionless unit universally used in computed tomography (CT) scanning to express CT numbers in a standardized and convenient form. Well, Hounsfield Units is related to the composition and nature of the tissue and thus, represent the density of various tissues. Note that the higher the HU value the more dense a material is and vice-versa. Below are a few values on the HU Scale that are important to keep in mind while analyzing CT images. Window Width and Window Level Window Width (WW) The window Width (WW) as the name suggests is the measure of the range of CT numbers that a CT image contains. Window width refers to the number of different HU units that will be represented by different shades of gray. Any HU value that falls below the lower value of the window width will show up as black on the scan while any HU value that is above the upper value of the window width will be white. Window Level (WL) The Window Level (WL) refers to the window centre or midpoint HU value that is represented on the window setting. The lower the window level is set for an image, the brighter the entire image will become. Windowing settings Understanding the difference between window width and window level is not enough and thus, let’s look at how changing these values can have a significant impact on image interpretation. Adjusting the Window Width As the width of the window increases a larger change in density will be required to change the shade of grey that represents a certain HU unit. This results in a loss of contrast as more structures will appear similar (despite having different densities) as there is an increase in the range of CT numbers. Hence, increasing the window width will decrease the contrast of the image. Adjusting the Window Width Now, as the width of the window decreases, a smaller change in density will result in a change in color on the study images. This will give increased contrast as structures that are close in density will have different shades of grey or white or black assigned to them. Hence, decreasing the window width will increase the contrast of the image. Impact of window width on the appearnace of a CT scan. Adjusting the Window Level As the window level is increased, the image will get darker. This is because as the level is increased, higher HU values will be required in order for a density to be represented as white and many lower values will be ignored (considered as black). Hence, increasing the window level will decrease the brightness of the image. Adjusting the Window Level Now, as the window level is increased, the image will get darker. This is because as the level is decreased, lower HU values will be required in order for a density to be represented as white, and thus, more white values will be allowed to pass which makes the image brighter. Hence, decreasing the window level will increase the brightness of the image. Impact of window level on the appearnace of a CT scan. Typical window settings Below is a list of various window settings that may somewhat vary from institution to institution. However, they are generally fairly similar. Here, the numbers shown in the figure represent two different values, the first being the window width (WW) and the second being window level (wl). Bone Window As the name suggests, a bone window is useful for viewing the bones. A high window level near the density of bone (given its density the level is high) is used with a wide window to give a good resolution. Window settings: (W:2000, L:500) or (W:3000, L:1000) Lung Window This window is used to evaluate the lungs. A high window level near the density of lung tissue (given its low density the level is low) and is used with a wide window to give good resolution and to visualize a wide variety of densities in the chest such as the lung parenchyma as well as adjacent blood vessels. Window settings: (W:1600, L:-600) or (W:1500, L:-500) Abdomen Window The abdomen window is used to evaluate the abdominal cavity and its contents. A window level close to fluid or soft tissue is used with a medium-sized window that gives a moderate amount of contrast. Window settings: (W:400, L:40) Brain Window As the name suggests, the brain window is designed to evaluate the brain parenchyma (typically on a non-contrast scan) and is especially sensitive at picking up intracranial hemorrhage with a narrow window and a level close to the density of expected hemorrhage in the brain. Window settings: (W:70, L:30) or (W:70, L:35) Soft Tissue Window As the name suggests, this window is used to evaluate soft tissues. The window level is set at the density of soft tissues (50 HU) and a moderate-sized window is used to give a balance between contrast and resolution. Window settings: (W:350, L:50) Liver Window This window is similar to the abdominal window however utilizes a narrower window to increase the contrast in the liver parenchyma (in order to make finding hepatic lesions a bit easier). Window settings: (W:160, L:60) Reference Fundamentals Of Computed Tomography Studies: Windowing</summary></entry><entry><title type="html">fastText</title><link href="http://localhost:4000/fastText" rel="alternate" type="text/html" title="fastText" /><published>2020-11-19T12:00:00+05:30</published><updated>2020-11-19T12:00:00+05:30</updated><id>http://localhost:4000/fastText</id><content type="html" xml:base="http://localhost:4000/fastText">&lt;p&gt;FastText is an open-source, free, lightweight library that allows users to learn text/word representations and text classifiers.&lt;/p&gt;

&lt;p&gt;The major benefits of using fastText are that it works on standard, generic hardware and the models can later be reduced in size to even fit on mobile devices.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Most of the techniques represent each word of the vocabulary with a distinct vector i.e. without a shared parameter between words. In other words, they ignore the internal structure of words which might affect the learning of languages rich in morphology.&lt;/p&gt;

&lt;p&gt;Thus, &lt;a href=&quot;https://arxiv.org/pdf/1607.04606.pdf&quot;&gt;Enriching Word Vectors with Subword Information&lt;/a&gt; proposes an alternative approach where they learn representations for character &lt;em&gt;n&lt;/em&gt;-grams and represent words as the sum of the &lt;em&gt;n&lt;/em&gt;-gram vectors.&lt;/p&gt;

&lt;h2 id=&quot;experimental-setup&quot;&gt;Experimental setup&lt;/h2&gt;

&lt;h3 id=&quot;subword-model&quot;&gt;Subword model&lt;/h3&gt;

&lt;p&gt;The proposed model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sisg&lt;/code&gt; (Subword Information Skip Gram) is based on the continuous skipgram model introduced by Mikolov et al. (2013b).&lt;/p&gt;

&lt;p&gt;Since the base skipgram model ignores the internal structure of words by using a distinct word representation for each word, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sisg&lt;/code&gt; proposes a different scoring function &lt;em&gt;s&lt;/em&gt;, in order to take into account the internal structure information.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/fasttext/1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where each word &lt;em&gt;w&lt;/em&gt; is represented as a bag of character &lt;em&gt;n&lt;/em&gt;-gram, &lt;em&gt;G&lt;sub&gt;w&lt;/sub&gt;&lt;/em&gt; belongs to the set { 1, …, &lt;em&gt;G&lt;/em&gt; } of the &lt;em&gt;n&lt;/em&gt;-grams of size &lt;em&gt;G&lt;/em&gt;. Each &lt;em&gt;n&lt;/em&gt;-gram &lt;em&gt;g&lt;/em&gt; is associated to a vector represention &lt;strong&gt;z&lt;/strong&gt;&lt;sub&gt;g&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Now, for example, the word &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;where&lt;/code&gt; with n=3 will be represented by the character &lt;em&gt;n&lt;/em&gt;-grams as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;wh, whe, her, ere, re&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;and the special sequence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;where&amp;gt;&lt;/code&gt;. Here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt; are added as special boundary symbols to distinguish prefixes and suffixes from other characters sequences.&lt;/p&gt;

&lt;h3 id=&quot;optimization&quot;&gt;Optimization&lt;/h3&gt;

&lt;p&gt;The optimization problem is solved by using stochastic gradient descent on the negative log-likelihood function. The optimization is being carried out in parallel where all threads share parameters and update vectors in an asynchronous manner.&lt;/p&gt;

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation details&lt;/h3&gt;

&lt;p&gt;Coming to implementation details, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sisg&lt;/code&gt; has word vectors of dimension 300 where 5 negatives are sampled at random for each positive example. The context window of size &lt;em&gt;c&lt;/em&gt; lies between 1 and 5. The step size is set to 0.05 since this is the default value set in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;word2vec&lt;/code&gt; package and works well for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sisg&lt;/code&gt; model too.&lt;/p&gt;

&lt;p&gt;Also, while building the word dictionary, only those words were kept which appeared at least 5 times in the training set.&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sisg&lt;/code&gt; model was trained on &lt;a href=&quot;https://dumps.wikimedia.org/&quot;&gt;Wikipedia data&lt;/a&gt; which consists of nine languages. The Wikipedia data was pre-processed using a &lt;a href=&quot;http://mattmahoney.net/dc/textdata&quot;&gt;perl script&lt;/a&gt;. All the datasets are shuffled and used to train the model over 5 passes.&lt;/p&gt;

&lt;p&gt;Now because of the simplicity, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sisg&lt;/code&gt; model trains fast and does not require heavy preprocessing or supervision.&lt;/p&gt;

&lt;h2 id=&quot;text-classification-with-fasttext&quot;&gt;Text classification with fastText&lt;/h2&gt;

&lt;p&gt;Text classification is a core problem to many applications and the fastText tool helps us easily solve this problem.&lt;/p&gt;

&lt;h3 id=&quot;installation&quot;&gt;Installation&lt;/h3&gt;

&lt;p&gt;Download and unzip the most recent fastText release:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip
$ unzip v0.9.2.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Move to the fastText directory and install as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cd fastText-0.9.2
 
# to install using the command-line tool
$ make
 
# to install via python bindings (we select this approach)
$ pip install .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We check the installation by importing fastText in a Python console:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import fasttext
&amp;gt;&amp;gt;&amp;gt; help(fasttext.FastText)
Help on module fasttext.FastText in fasttext:
 
NAME
    fasttext.FastText
 
DESCRIPTION
    # Copyright (c) 2017-present, Facebook, Inc.
    # All rights reserved.
    #
    # This source code is licensed under the MIT license found in the
    # LICENSE file in the root directory of this source tree.
 
FUNCTIONS
    load_model(path)
        Load a model given a filepath and return a model object.
 
    read_args(arg_list, arg_dict, arg_names, default_values)
 
    tokenize(text)
        Given a string of text, tokenize it and return a list of tokens
 
    train_supervised(*kargs, **kwargs)
        Train a supervised model and return a model object.
 
        input must be a filepath. The input text does not need to be tokenized
        as per the tokenize function, but it must be preprocessed and encoded
        as UTF-8. You might want to consult standard preprocessing scripts such
        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html
 
        The input file must must contain at least one label per line. For an
        example consult the example datasets which are part of the fastText
        repository such as the dataset pulled by classification-example.sh.
 
    train_unsupervised(*kargs, **kwargs)
        Train an unsupervised model and return a model object.
 
        input must be a filepath. The input text does not need to be tokenized
        as per the tokenize function, but it must be preprocessed and encoded
        as UTF-8. You might want to consult standard preprocessing scripts such
        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html
 
        The input field must not contain any labels or use the specified label prefix
        unless it is ok for those words to be ignored. For an example consult the
        dataset pulled by the example script word-vector-example.sh, which is
        part of the fastText repository.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;dataset-1&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;Let’s download example questions from the &lt;a href=&quot;https://cooking.stackexchange.com/&quot;&gt;Stackexchange&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz &amp;amp;&amp;amp; tar xvzf cooking.stackexchange.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Before training a text classifier we need to split the dataset into training and validation sets. We use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wc&lt;/code&gt; command to check the number of lines in the dataset:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ wc cooking.stackexchange.txt
15404  169582 1401900 cooking.stackexchange.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The dataset contains 15404 lines i.e. 15404 examples which we split into a training set of 12404 examples and a validation set of 3000 examples:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ head -n 12404 cooking.stackexchange.txt &amp;gt; cooking.train
$ tail -n 3000 cooking.stackexchange.txt &amp;gt; cooking.valid
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;train&quot;&gt;Train&lt;/h3&gt;

&lt;p&gt;To train the text classifier we import fastText and then use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_supervised&lt;/code&gt; method by providing the training set as an input parameter.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import fasttext
&amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input=&quot;cooking.train&quot;)
Read 0M words
Number of words:  8974
Number of labels: 735
Progress: 100.0% words/sec/thread:   77120 lr:  0.000000 avg.loss:  9.961853 ETA:   0h 0m 0s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;save&quot;&gt;Save&lt;/h3&gt;

&lt;p&gt;We save the model with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;save_model&lt;/code&gt; so that we can load it later with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_model&lt;/code&gt; function:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; model.save_model(&quot;model_cooking.bin&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;test&quot;&gt;Test&lt;/h3&gt;

&lt;p&gt;We can test the model as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; model.predict(&quot;Which baking dish is best to bake a banana bread ?&quot;)
(('__label__baking',), array([0.21342881]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predict&lt;/code&gt; method predicts &lt;em&gt;baking&lt;/em&gt; tag for the given text input. Let’s look at another example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; model.predict(&quot;Why not put knives in the dishwasher?&quot;)
(('__label__food-safety',), array([0.09138963]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The label predicted in this case is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;food-safety&lt;/code&gt; which is not relevant for the given input. To get a better understanding, let’s test the model on the validation set:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;)
(3000, 0.172, 0.07438373936860314)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output contains the number of samples (3000), the precision at one (0.172), and the recall at one (0.074).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is precision?&lt;/p&gt;

  &lt;p&gt;Precision is a measure of how precise or accurate the model is out of those predicted positive, how many of them are actually positive.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/img/fasttext/2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is recall?&lt;/p&gt;

  &lt;p&gt;Recall is a measure that calculates how many of the Actual Positives of the model are True Positives.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/img/fasttext/3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We can also compute the precision and recall at &lt;em&gt;k&lt;/em&gt; (here we use k=5) as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;, k=5)
(3000, 0.07286666666666666, 0.1575609052904714)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;optimization-1&quot;&gt;Optimization&lt;/h3&gt;

&lt;p&gt;We can optimize and improve the performance of the model by performing various steps given below&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Preprocessing the dataset&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The raw dataset usually contains elements like uppercase letters or punctuations which are not required for training and might/might not affect the model’s performance. Thus, we can normalize the dataset by using command line tools such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sed&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tr&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; $ cat cooking.stackexchange.txt | sed -e &quot;s/\([.\!?,'/()]\)/ \1 /g&quot; | tr &quot;[:upper:]&quot; &quot;[:lower:]&quot; &amp;gt; cooking.preprocessed.txt
 $ head -n 12404 cooking.preprocessed.txt &amp;gt; cooking.train
 $ tail -n 3000 cooking.preprocessed.txt &amp;gt; cooking.valid
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, we retrain our model on the preprocessed dataset:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input=&quot;cooking.train&quot;)
Read 0M words
Number of words:  8952
Number of labels: 735
Progress: 100.0% words/sec/thread:   46336 lr:  0.000000 avg.loss: 10.019582 ETA:   0h 0m 0s
 
&amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;)
(3000, 0.17466666666666666, 0.07553697563788381)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can observe a slight improvement in the results which can be significant in other cases.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tweaking number of epochs and learning rate&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;fastText sees each training example only 5 times (epochs=5) by default which may be pretty small depending on the size of the dataset. We can change this by using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;epoch&lt;/code&gt; option while training.&lt;/p&gt;

&lt;p&gt;Also, the learning rate of the model corresponds to how much the model changes after processing each example and we can tweak it by using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lr&lt;/code&gt; option.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input=&quot;cooking.train&quot;, lr=1.0, epoch=25)
Read 0M words
Number of words:  8952
Number of labels: 735
Progress: 100.0% words/sec/thread:   60929 lr:  0.000000 avg.loss:  4.399605 ETA:   0h 0m 0s
 
&amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;)
(3000, 0.585, 0.25299120657344676)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We observe drastic changes in the output results and thus, it is evident that experimenting with hyperparameters such as learning rate and epochs can significantly improve a model’s performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;n&lt;/em&gt;-grams&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Currently, we use unigrams for training the model which generally does not help much. Instead, we can use bigrams which might cover prefixes or suffixes properly. In bigrams, we split a sentence or corpus of text into 2 tokens or words, unlike unigrams.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input=&quot;cooking.train&quot;, lr=1.0, epoch=25, wordNgrams=2)
Read 0M words
Number of words:  8952
Number of labels: 735
Progress: 100.0% words/sec/thread:   66974 lr:  0.000000 avg.loss:  3.152711 ETA:   0h 0m 0s
 
&amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;)
(3000, 0.6083333333333333, 0.2630820239296526)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The results have further improved with just a single easy step.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hierarchical Softmax&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Finally, we replace the regular softmax function with a hierarchical softmax function for loss since it helps training models on large datasets faster.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input=&quot;cooking.train&quot;, lr=1.0, epoch=25, wordNgrams=2, bucket=200000, dim=50, loss='hs')Read 0M words
Number of words:  8952
Number of labels: 735
Progress: 100.0% words/sec/thread:  899564 lr:  0.000000 avg.loss:  2.271247 ETA:   0h 0m 0s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bucket&lt;/code&gt; is used to define the bucket size and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dim&lt;/code&gt; is the dimension of the word vectors.&lt;/p&gt;

&lt;h3 id=&quot;autotune&quot;&gt;Autotune&lt;/h3&gt;

&lt;p&gt;We observed that finding the best hyperparameters is crucial for building efficient models but doing it manually is difficult. This is where fastText’s autotune feature comes to help.&lt;/p&gt;

&lt;p&gt;FastText’s autotune feature allows you to automatically perform hyperparameter optimization for the model by providing a validation file with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autottuneValidationFile&lt;/code&gt; parameter.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input='cooking.train', autotuneValidationFile='cooking.valid')
Progress: 100.0% Trials:   12 Best score:  0.335514 ETA:   0h 0m 0s
Training again with best arguments
Read 0M words
Number of words:  8952
Number of labels: 735
Progress: 100.0% words/sec/thread:   66732 lr:  0.000000 avg.loss:  4.540132 ETA:   0h 0m 0s
 
&amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;)
(3000, 0.5583333333333333, 0.24145884388064004)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We get the best F1-score in the output after a default duration of 5 minutes which can be changed by setting the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autotuneDuration&lt;/code&gt; parameter.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is F1-score?&lt;/p&gt;

  &lt;p&gt;F1-score is a function of Precision and Recall as shown below. F1-score is an important measure that is required to seek a proper balance between Precision and Recall.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/img/fasttext/4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://fasttext.cc/docs/en/supervised-tutorial.html&quot;&gt;Text classification&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Keval Nagda</name></author><category term="jekyll" /><category term="update" /><summary type="html">FastText is an open-source, free, lightweight library that allows users to learn text/word representations and text classifiers. The major benefits of using fastText are that it works on standard, generic hardware and the models can later be reduced in size to even fit on mobile devices. Introduction Most of the techniques represent each word of the vocabulary with a distinct vector i.e. without a shared parameter between words. In other words, they ignore the internal structure of words which might affect the learning of languages rich in morphology. Thus, Enriching Word Vectors with Subword Information proposes an alternative approach where they learn representations for character n-grams and represent words as the sum of the n-gram vectors. Experimental setup Subword model The proposed model sisg (Subword Information Skip Gram) is based on the continuous skipgram model introduced by Mikolov et al. (2013b). Since the base skipgram model ignores the internal structure of words by using a distinct word representation for each word, sisg proposes a different scoring function s, in order to take into account the internal structure information. where each word w is represented as a bag of character n-gram, Gw belongs to the set { 1, …, G } of the n-grams of size G. Each n-gram g is associated to a vector represention zgT. Now, for example, the word where with n=3 will be represented by the character n-grams as &amp;lt;wh, whe, her, ere, re&amp;gt;. and the special sequence &amp;lt;where&amp;gt;. Here &amp;lt; and &amp;gt; are added as special boundary symbols to distinguish prefixes and suffixes from other characters sequences. Optimization The optimization problem is solved by using stochastic gradient descent on the negative log-likelihood function. The optimization is being carried out in parallel where all threads share parameters and update vectors in an asynchronous manner. Implementation details Coming to implementation details, sisg has word vectors of dimension 300 where 5 negatives are sampled at random for each positive example. The context window of size c lies between 1 and 5. The step size is set to 0.05 since this is the default value set in the word2vec package and works well for sisg model too. Also, while building the word dictionary, only those words were kept which appeared at least 5 times in the training set. Dataset The sisg model was trained on Wikipedia data which consists of nine languages. The Wikipedia data was pre-processed using a perl script. All the datasets are shuffled and used to train the model over 5 passes. Now because of the simplicity, the sisg model trains fast and does not require heavy preprocessing or supervision. Text classification with fastText Text classification is a core problem to many applications and the fastText tool helps us easily solve this problem. Installation Download and unzip the most recent fastText release: $ wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip $ unzip v0.9.2.zip Move to the fastText directory and install as follows: $ cd fastText-0.9.2 # to install using the command-line tool $ make # to install via python bindings (we select this approach) $ pip install . We check the installation by importing fastText in a Python console: &amp;gt;&amp;gt;&amp;gt; import fasttext &amp;gt;&amp;gt;&amp;gt; help(fasttext.FastText) Help on module fasttext.FastText in fasttext: NAME fasttext.FastText DESCRIPTION # Copyright (c) 2017-present, Facebook, Inc. # All rights reserved. # # This source code is licensed under the MIT license found in the # LICENSE file in the root directory of this source tree. FUNCTIONS load_model(path) Load a model given a filepath and return a model object. read_args(arg_list, arg_dict, arg_names, default_values) tokenize(text) Given a string of text, tokenize it and return a list of tokens train_supervised(*kargs, **kwargs) Train a supervised model and return a model object. input must be a filepath. The input text does not need to be tokenized as per the tokenize function, but it must be preprocessed and encoded as UTF-8. You might want to consult standard preprocessing scripts such as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html The input file must must contain at least one label per line. For an example consult the example datasets which are part of the fastText repository such as the dataset pulled by classification-example.sh. train_unsupervised(*kargs, **kwargs) Train an unsupervised model and return a model object. input must be a filepath. The input text does not need to be tokenized as per the tokenize function, but it must be preprocessed and encoded as UTF-8. You might want to consult standard preprocessing scripts such as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html The input field must not contain any labels or use the specified label prefix unless it is ok for those words to be ignored. For an example consult the dataset pulled by the example script word-vector-example.sh, which is part of the fastText repository. Dataset Let’s download example questions from the Stackexchange: $ wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz &amp;amp;&amp;amp; tar xvzf cooking.stackexchange.tar.gz Before training a text classifier we need to split the dataset into training and validation sets. We use wc command to check the number of lines in the dataset: $ wc cooking.stackexchange.txt 15404 169582 1401900 cooking.stackexchange.txt The dataset contains 15404 lines i.e. 15404 examples which we split into a training set of 12404 examples and a validation set of 3000 examples: $ head -n 12404 cooking.stackexchange.txt &amp;gt; cooking.train $ tail -n 3000 cooking.stackexchange.txt &amp;gt; cooking.valid Train To train the text classifier we import fastText and then use the train_supervised method by providing the training set as an input parameter. &amp;gt;&amp;gt;&amp;gt; import fasttext &amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input=&quot;cooking.train&quot;) Read 0M words Number of words: 8974 Number of labels: 735 Progress: 100.0% words/sec/thread: 77120 lr: 0.000000 avg.loss: 9.961853 ETA: 0h 0m 0s Save We save the model with save_model so that we can load it later with load_model function: &amp;gt;&amp;gt;&amp;gt; model.save_model(&quot;model_cooking.bin&quot;) Test We can test the model as follows: &amp;gt;&amp;gt;&amp;gt; model.predict(&quot;Which baking dish is best to bake a banana bread ?&quot;) (('__label__baking',), array([0.21342881])) The predict method predicts baking tag for the given text input. Let’s look at another example: &amp;gt;&amp;gt;&amp;gt; model.predict(&quot;Why not put knives in the dishwasher?&quot;) (('__label__food-safety',), array([0.09138963])) The label predicted in this case is food-safety which is not relevant for the given input. To get a better understanding, let’s test the model on the validation set: &amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;) (3000, 0.172, 0.07438373936860314) The output contains the number of samples (3000), the precision at one (0.172), and the recall at one (0.074). What is precision? Precision is a measure of how precise or accurate the model is out of those predicted positive, how many of them are actually positive. What is recall? Recall is a measure that calculates how many of the Actual Positives of the model are True Positives. We can also compute the precision and recall at k (here we use k=5) as follows: &amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;, k=5) (3000, 0.07286666666666666, 0.1575609052904714) Optimization We can optimize and improve the performance of the model by performing various steps given below Preprocessing the dataset The raw dataset usually contains elements like uppercase letters or punctuations which are not required for training and might/might not affect the model’s performance. Thus, we can normalize the dataset by using command line tools such as sed and tr: $ cat cooking.stackexchange.txt | sed -e &quot;s/\([.\!?,'/()]\)/ \1 /g&quot; | tr &quot;[:upper:]&quot; &quot;[:lower:]&quot; &amp;gt; cooking.preprocessed.txt $ head -n 12404 cooking.preprocessed.txt &amp;gt; cooking.train $ tail -n 3000 cooking.preprocessed.txt &amp;gt; cooking.valid Now, we retrain our model on the preprocessed dataset: &amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input=&quot;cooking.train&quot;) Read 0M words Number of words: 8952 Number of labels: 735 Progress: 100.0% words/sec/thread: 46336 lr: 0.000000 avg.loss: 10.019582 ETA: 0h 0m 0s &amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;) (3000, 0.17466666666666666, 0.07553697563788381) We can observe a slight improvement in the results which can be significant in other cases. Tweaking number of epochs and learning rate fastText sees each training example only 5 times (epochs=5) by default which may be pretty small depending on the size of the dataset. We can change this by using the epoch option while training. Also, the learning rate of the model corresponds to how much the model changes after processing each example and we can tweak it by using the lr option. &amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input=&quot;cooking.train&quot;, lr=1.0, epoch=25) Read 0M words Number of words: 8952 Number of labels: 735 Progress: 100.0% words/sec/thread: 60929 lr: 0.000000 avg.loss: 4.399605 ETA: 0h 0m 0s &amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;) (3000, 0.585, 0.25299120657344676) We observe drastic changes in the output results and thus, it is evident that experimenting with hyperparameters such as learning rate and epochs can significantly improve a model’s performance. n-grams Currently, we use unigrams for training the model which generally does not help much. Instead, we can use bigrams which might cover prefixes or suffixes properly. In bigrams, we split a sentence or corpus of text into 2 tokens or words, unlike unigrams. &amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input=&quot;cooking.train&quot;, lr=1.0, epoch=25, wordNgrams=2) Read 0M words Number of words: 8952 Number of labels: 735 Progress: 100.0% words/sec/thread: 66974 lr: 0.000000 avg.loss: 3.152711 ETA: 0h 0m 0s &amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;) (3000, 0.6083333333333333, 0.2630820239296526) The results have further improved with just a single easy step. Hierarchical Softmax Finally, we replace the regular softmax function with a hierarchical softmax function for loss since it helps training models on large datasets faster. &amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input=&quot;cooking.train&quot;, lr=1.0, epoch=25, wordNgrams=2, bucket=200000, dim=50, loss='hs')Read 0M words Number of words: 8952 Number of labels: 735 Progress: 100.0% words/sec/thread: 899564 lr: 0.000000 avg.loss: 2.271247 ETA: 0h 0m 0s Here, bucket is used to define the bucket size and dim is the dimension of the word vectors. Autotune We observed that finding the best hyperparameters is crucial for building efficient models but doing it manually is difficult. This is where fastText’s autotune feature comes to help. FastText’s autotune feature allows you to automatically perform hyperparameter optimization for the model by providing a validation file with the autottuneValidationFile parameter. &amp;gt;&amp;gt;&amp;gt; model = fasttext.train_supervised(input='cooking.train', autotuneValidationFile='cooking.valid') Progress: 100.0% Trials: 12 Best score: 0.335514 ETA: 0h 0m 0s Training again with best arguments Read 0M words Number of words: 8952 Number of labels: 735 Progress: 100.0% words/sec/thread: 66732 lr: 0.000000 avg.loss: 4.540132 ETA: 0h 0m 0s &amp;gt;&amp;gt;&amp;gt; model.test(&quot;cooking.test&quot;) (3000, 0.5583333333333333, 0.24145884388064004) We get the best F1-score in the output after a default duration of 5 minutes which can be changed by setting the autotuneDuration parameter. What is F1-score? F1-score is a function of Precision and Recall as shown below. F1-score is an important measure that is required to seek a proper balance between Precision and Recall. Reference Text classification</summary></entry><entry><title type="html">Build and Publish Docker image using Jenkins</title><link href="http://localhost:4000/jenkins-git-docker-image" rel="alternate" type="text/html" title="Build and Publish Docker image using Jenkins" /><published>2020-11-18T12:00:00+05:30</published><updated>2020-11-18T12:00:00+05:30</updated><id>http://localhost:4000/jenkins-git-docker-image</id><content type="html" xml:base="http://localhost:4000/jenkins-git-docker-image">&lt;p&gt;Today we’re going to learn how to build a Docker image using Jenkinsfile from a git repository and push it to the Docker Hub.&lt;/p&gt;

&lt;h2 id=&quot;create-a-new-jenkins-docker-image&quot;&gt;Create a new Jenkins Docker image&lt;/h2&gt;

&lt;p&gt;The official Jenkins image does not have docker installed in it. So if you try to access docker while running a container based on the official Jenkins image it would result in an error.&lt;/p&gt;

&lt;p&gt;How to solve this? we can create a new Jenkins Docker image by preinstalling Docker in it. Following is the Dockerfile that we use to create the new Jenkins Docker image.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM jenkins/jenkins:latest
 
USER root
RUN apt-get update -qq \
    &amp;amp;&amp;amp; apt-get install -qqy apt-transport-https ca-certificates curl gnupg2 software-properties-common
RUN curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -
RUN add-apt-repository \
  &quot;deb [arch=amd64] https://download.docker.com/linux/debian \
  $(lsb_release -cs) \
  stable&quot;
RUN apt-get update  -qq \
    &amp;amp;&amp;amp; apt-get install docker-ce=17.12.1~ce-0~debian -y
 
RUN usermod -aG docker jenkins
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we use the base image as Jenkins official image, download and install Docker on top of it. Later we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;usermod&lt;/code&gt; command to change attributes of the docker and jenkins group.&lt;/p&gt;

&lt;p&gt;Next, create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file to ease the process of Docker image creation.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;version: '3'
 
services:
  jenkins:
    container_name: 'jenkins-container'
    privileged: true
    build: .
    ports:
      - '8080:8080'
      - '50000:50000'
    volumes:
      - myjenkins:/var/jenkins_home
      - /var/run/docker.sock:/var/run/docker.sock
    restart: unless-stopped
 
volumes:
  myjenkins:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, we mount a Docker volume &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;myjenkins&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/jenkins_home&lt;/code&gt; directory which lies inside the Docker container and we also map the Docker socket from host to the container.&lt;/p&gt;

&lt;p&gt;Build and run the Docker image by executing the following command in the project directory.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker-compose up
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;set-up-jenkins&quot;&gt;Set up Jenkins&lt;/h2&gt;

&lt;p&gt;Once Jenkins files have been extracted, the Jenkins server will be fully up and running at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:8080&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can find the initial admin password at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/jenkins_home/secrets/initialAdminPassword&lt;/code&gt; as mentioned on the login page.&lt;/p&gt;

&lt;p&gt;Next, we can install plugins as per our requirement.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It may take some time depending upon the number of plugins you choose to install. Once the plugins are installed, you will be prompted to create a first admin user which you can skip if you wish to continue as an admin user.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;create-jenkins-job&quot;&gt;Create Jenkins job&lt;/h2&gt;

&lt;p&gt;On completion of the initial setup, create a new pipeline in Jenkins by selecting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;New Item&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Enter the name of the job and select the type of job you wish to run on Jenkins. We select the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pipeline&lt;/code&gt; option since we wish to create a Jenkins pipeline to execute a series of steps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are multiple options as triggers for Jenkins, however, we use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Polling&lt;/code&gt; method and set a schedule as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;* * * * *&lt;/code&gt; which will poll the SCM repository every minute.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now in the Pipeline section, select the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pipeline script from SCM&lt;/code&gt; option, select SCM, and insert the URL of the SCM repository.&lt;/p&gt;

&lt;p&gt;You can add credentials for authentication however, credentials are not required for repositories with public access.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can also select a specific branch that you wish to build by adding the branch name in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Branch to build&lt;/code&gt; section. For example, add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*/main&lt;/code&gt; to build the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt; branch.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Save&lt;/code&gt; button and go to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Plugin Manager&lt;/code&gt; to install the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Docker Build and Publish&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Docker Pipeline&lt;/code&gt; plugin which helps us to build and push the Docker image to Docker Hub.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;add-jenkinsfile&quot;&gt;Add Jenkinsfile&lt;/h2&gt;

&lt;p&gt;Once the plugin has been installed, go ahead and add a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Jenkinsfile&lt;/code&gt; script given below to the SCM repository which will be used by Jenkins while building a job.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pipeline {
  environment {
    imagename = &quot;kevalnagda/flaskapp&quot;
    registryCredential = 'kevalnagda'
    dockerImage = ''
  }
  agent any
  stages {
    stage('Cloning Git') {
      steps {
        git([url: 'https://github.com/kevalnagda/movieapp.git', branch: 'main', credentialsId: 'kevalnagda'])
 
      }
    }
    stage('Building image') {
      steps{
        script {
          dockerImage = docker.build imagename
        }
      }
    }
    stage('Deploy Image') {
      steps{
        script {
          docker.withRegistry( '', registryCredential ) {
            dockerImage.push(&quot;$BUILD_NUMBER&quot;)
             dockerImage.push('latest')
          }
        }
      }
    }
    stage('Remove Unused docker image') {
      steps{
        sh &quot;docker rmi $imagename:$BUILD_NUMBER&quot;
         sh &quot;docker rmi $imagename:latest&quot;
 
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;testing&quot;&gt;Testing&lt;/h2&gt;

&lt;p&gt;Now, commit changes to the SCM repository to test and see if Jenkins can access the SCM repository and Jenkinsfile.&lt;/p&gt;

&lt;p&gt;On successful completion of the job, you would be able to see the latest Docker image in your Docker Hub repository.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name>Keval Nagda</name></author><category term="jekyll" /><category term="update" /><summary type="html">Today we’re going to learn how to build a Docker image using Jenkinsfile from a git repository and push it to the Docker Hub. Create a new Jenkins Docker image The official Jenkins image does not have docker installed in it. So if you try to access docker while running a container based on the official Jenkins image it would result in an error. How to solve this? we can create a new Jenkins Docker image by preinstalling Docker in it. Following is the Dockerfile that we use to create the new Jenkins Docker image. FROM jenkins/jenkins:latest USER root RUN apt-get update -qq \ &amp;amp;&amp;amp; apt-get install -qqy apt-transport-https ca-certificates curl gnupg2 software-properties-common RUN curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - RUN add-apt-repository \ &quot;deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable&quot; RUN apt-get update -qq \ &amp;amp;&amp;amp; apt-get install docker-ce=17.12.1~ce-0~debian -y RUN usermod -aG docker jenkins Here we use the base image as Jenkins official image, download and install Docker on top of it. Later we use usermod command to change attributes of the docker and jenkins group. Next, create a docker-compose.yml file to ease the process of Docker image creation. version: '3' services: jenkins: container_name: 'jenkins-container' privileged: true build: . ports: - '8080:8080' - '50000:50000' volumes: - myjenkins:/var/jenkins_home - /var/run/docker.sock:/var/run/docker.sock restart: unless-stopped volumes: myjenkins: Here, we mount a Docker volume myjenkins to /var/jenkins_home directory which lies inside the Docker container and we also map the Docker socket from host to the container. Build and run the Docker image by executing the following command in the project directory. docker-compose up Set up Jenkins Once Jenkins files have been extracted, the Jenkins server will be fully up and running at http://localhost:8080. You can find the initial admin password at /var/jenkins_home/secrets/initialAdminPassword as mentioned on the login page. Next, we can install plugins as per our requirement. It may take some time depending upon the number of plugins you choose to install. Once the plugins are installed, you will be prompted to create a first admin user which you can skip if you wish to continue as an admin user.</summary></entry><entry><title type="html">AMD CPU review</title><link href="http://localhost:4000/amd-cpu-review" rel="alternate" type="text/html" title="AMD CPU review" /><published>2020-11-12T12:00:00+05:30</published><updated>2020-11-12T12:00:00+05:30</updated><id>http://localhost:4000/amd-cpu-review</id><content type="html" xml:base="http://localhost:4000/amd-cpu-review">&lt;p&gt;AMD’s mainstream Ryzen chips are highly disruptive which include several families of various levels of potency. Ryzen chips introduce a completely new motherboard platform, and the processors require different memory and coolers than their predecessors.&lt;/p&gt;

&lt;p&gt;Below is a quick breakdown of the different AMD Ryzen processor brackets:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ryzen 3: Up to 4-core processors.&lt;/li&gt;
  &lt;li&gt;Ryzen 5: Up to 8-core processors.&lt;/li&gt;
  &lt;li&gt;Ryzen 7: Up to 16-core processors.&lt;/li&gt;
  &lt;li&gt;Ryzen 9: Up to 32-core processors.&lt;/li&gt;
  &lt;li&gt;Threadripper: Up to 64-core processors.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Within each bracket, the processors are named by model number - the higher the model number the more powerful the CPU.&lt;/p&gt;

&lt;p&gt;Ryzen CPUs that start with the number two are Zen+ processors, released in 2018 while the third generation CPUs using Zen 2 were released in 2019.&lt;/p&gt;

&lt;p&gt;Now, AMD also has CPUs with G as a suffix for graphics which include Vega video processing which means that you can run a system without a dedicated video card.&lt;/p&gt;

&lt;p&gt;Coming to socket type, all AMD processors use the same AM4 socket with a few exceptions like Generation 1 and 2 Threadrippers using a TR4 socket and Generation 3 Threadrippers using a TRX4 socket.&lt;/p&gt;

&lt;h2 id=&quot;ryzen-3&quot;&gt;Ryzen 3&lt;/h2&gt;

&lt;p&gt;The Ryzen 3 processors are built for budget-friendly PCs and consumers who don’t use PCs for intensive tasks. All processors in this family i.e. 1000, 2000, and 3000 Series are quad-core with decent game handling capability.&lt;/p&gt;

&lt;p&gt;The latest generation chips in this family come with a rocking boost clock speed of 4.0GHz, allowing you to get good performance at the same affordable price.&lt;/p&gt;

&lt;h2 id=&quot;ryzen-5&quot;&gt;Ryzen 5&lt;/h2&gt;

&lt;p&gt;The Ryzen 5 processors are incredible for gaming and are priced aggressively to take on the popular Intel Core i5 family. These processors contain a mix of quadcore and hexacore processors, packed with enough power for intensive applications and tasks like video editing.&lt;/p&gt;

&lt;h2 id=&quot;ryzen-7&quot;&gt;Ryzen 7&lt;/h2&gt;

&lt;p&gt;The Ryzen 7 processors fall in line with Intel Core i7 processors and may be overkill for most people who don’t perform high computation tasks. However, these processors are extremely good for advanced computing in a somewhat affordable price range.&lt;/p&gt;

&lt;p&gt;All processors in this family have 8/16 core/thread configuration and the latest generation chips come with massive L3 cache, high clock speeds and great overclock support.&lt;/p&gt;

&lt;h2 id=&quot;ryzen-9&quot;&gt;Ryzen 9&lt;/h2&gt;

&lt;p&gt;The Ryzen 9 processors are the latest batch of processors that are magically packed with 12/24 to 16/32 core/thread configuration. These chips are amazing and lie in the middle-ground between Ryzen 7 and Threadripper.&lt;/p&gt;

&lt;p&gt;These processors challenge Intel’s Core i9 processor range and are optimal for gamers, streamers, and content creators.&lt;/p&gt;

&lt;h2 id=&quot;threadripper&quot;&gt;Threadripper&lt;/h2&gt;

&lt;p&gt;The Threadripper family is the ultimate CPU one can have which comes with upto 64 cores and 128 threads that allow advanced users to push their systems beyond the limit.&lt;/p&gt;</content><author><name>Keval Nagda</name></author><category term="jekyll" /><category term="update" /><summary type="html">AMD’s mainstream Ryzen chips are highly disruptive which include several families of various levels of potency. Ryzen chips introduce a completely new motherboard platform, and the processors require different memory and coolers than their predecessors. Below is a quick breakdown of the different AMD Ryzen processor brackets: Ryzen 3: Up to 4-core processors. Ryzen 5: Up to 8-core processors. Ryzen 7: Up to 16-core processors. Ryzen 9: Up to 32-core processors. Threadripper: Up to 64-core processors. Within each bracket, the processors are named by model number - the higher the model number the more powerful the CPU. Ryzen CPUs that start with the number two are Zen+ processors, released in 2018 while the third generation CPUs using Zen 2 were released in 2019. Now, AMD also has CPUs with G as a suffix for graphics which include Vega video processing which means that you can run a system without a dedicated video card. Coming to socket type, all AMD processors use the same AM4 socket with a few exceptions like Generation 1 and 2 Threadrippers using a TR4 socket and Generation 3 Threadrippers using a TRX4 socket. Ryzen 3 The Ryzen 3 processors are built for budget-friendly PCs and consumers who don’t use PCs for intensive tasks. All processors in this family i.e. 1000, 2000, and 3000 Series are quad-core with decent game handling capability. The latest generation chips in this family come with a rocking boost clock speed of 4.0GHz, allowing you to get good performance at the same affordable price. Ryzen 5 The Ryzen 5 processors are incredible for gaming and are priced aggressively to take on the popular Intel Core i5 family. These processors contain a mix of quadcore and hexacore processors, packed with enough power for intensive applications and tasks like video editing. Ryzen 7 The Ryzen 7 processors fall in line with Intel Core i7 processors and may be overkill for most people who don’t perform high computation tasks. However, these processors are extremely good for advanced computing in a somewhat affordable price range. All processors in this family have 8/16 core/thread configuration and the latest generation chips come with massive L3 cache, high clock speeds and great overclock support. Ryzen 9 The Ryzen 9 processors are the latest batch of processors that are magically packed with 12/24 to 16/32 core/thread configuration. These chips are amazing and lie in the middle-ground between Ryzen 7 and Threadripper. These processors challenge Intel’s Core i9 processor range and are optimal for gamers, streamers, and content creators. Threadripper The Threadripper family is the ultimate CPU one can have which comes with upto 64 cores and 128 threads that allow advanced users to push their systems beyond the limit.</summary></entry><entry><title type="html">How to build a Flask app with WSGI and Nginx</title><link href="http://localhost:4000/flask-app-with-wsgi-and-nginx" rel="alternate" type="text/html" title="How to build a Flask app with WSGI and Nginx" /><published>2020-11-10T12:00:00+05:30</published><updated>2020-11-10T12:00:00+05:30</updated><id>http://localhost:4000/flask-app-with-wsgi-and-nginx</id><content type="html" xml:base="http://localhost:4000/flask-app-with-wsgi-and-nginx">&lt;p&gt;In this blog, we learn how to build a movie quote generator flask application with Nginx using Gunicorn.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;h3 id=&quot;gunicorn&quot;&gt;Gunicorn&lt;/h3&gt;

&lt;p&gt;Gunicorn (Green Unicorn) is a Python Web Server Gateway Interface (WSGI) HTTP server. It is broadly compatible with various web frameworks.&lt;/p&gt;

&lt;h3 id=&quot;nginx&quot;&gt;Nginx&lt;/h3&gt;

&lt;p&gt;Nginx is an open-source HTTP web server, mail proxy server, and reverse proxy and load balancer for HTTP, TCP, and UDP traffic. Nginx provides high performance and stability with a simple configuration.&lt;/p&gt;

&lt;h3 id=&quot;why-use-gunicorn-and-nginx-with-flask&quot;&gt;Why use Gunicorn and Nginx with Flask?&lt;/h3&gt;

&lt;p&gt;Flask is just a web framework and not a web server. Thus to serve a flask application, a web server such as Gunicorn, Nginx or Apache is required to accept HTTP requests.&lt;/p&gt;

&lt;p&gt;Now, the major advantage of using Nginx and Gunicorn together is that in addition to being a web server, Nginx can also proxy connections to Gunicorn which brings good performance benefits along with the capability to handle a large number of connections with very little CPU usage and memory cost.&lt;/p&gt;

&lt;h2 id=&quot;build-the-flask-app&quot;&gt;Build the Flask app&lt;/h2&gt;

&lt;h3 id=&quot;1-update-and-install-local-packages&quot;&gt;1. Update and install local packages&lt;/h3&gt;

&lt;p&gt;First of all, update your local package index and then install the required packages as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Update the local package index
$ sudo apt-get update
 
# Install dependencies
$ sudo apt install python3-pip python3-dev python3-venv nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-create-a-virtual-environment&quot;&gt;2. Create a virtual environment&lt;/h3&gt;

&lt;p&gt;Now, we create a virtual environment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;env&lt;/code&gt; for the Python project using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;venv&lt;/code&gt; module.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 -m venv env
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We activate the environment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;env&lt;/code&gt; as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source env/bin/activate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-install-python-dependencies&quot;&gt;3. Install Python dependencies&lt;/h3&gt;

&lt;p&gt;Next, we install Flask and Gunicorn.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip3 install flask gunicorn requests
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4-create-the-movie-quotes-app&quot;&gt;4. Create the movie quotes app&lt;/h3&gt;

&lt;p&gt;We create a file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app.py&lt;/code&gt; with the following content in it:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import requests
from flask import Flask, render_template
app = Flask(__name__)
 
@app.route('/')
def movieapp():
   url = &quot;http://movie-quotes-2.herokuapp.com/api/v1/quotes/random&quot;   
   response = requests.get(url).json()
 
   return render_template(&quot;index.html&quot;, film=response['film'], quote=response['content'])
 
if __name__ == '__main__':
   app.run(debug=False, host='0.0.0.0')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;templates&lt;/code&gt; folder with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.html&lt;/code&gt; file in it to be used as a template for our flask application. Below is the sample &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.html&lt;/code&gt; file which can be modified as per your requirements.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&amp;lt;html&amp;gt;
 
&amp;lt;head&amp;gt;
   &amp;lt;style type=&quot;text/css&quot;&amp;gt;
       body {
           background: black;
           color: white;
       }
 
       div.container {
           max-width: 500px;
           margin: 100px auto;
           border: 20px solid white;
           padding: 10px;
           text-align: center;
       }
 
       h2 {
           text-transform: uppercase;
       }
   &amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
 
&amp;lt;body&amp;gt;
   &amp;lt;div class=&quot;container&quot;&amp;gt;
       &amp;lt;h2&amp;gt;Quote of the day&amp;lt;/h2&amp;gt;
       &amp;lt;hr&amp;gt;
       &amp;lt;h3&amp;gt;FILM: &amp;lt;/h3&amp;gt;&amp;lt;h4&amp;gt; {{ film }} &amp;lt;/h4&amp;gt;
       &amp;lt;h3&amp;gt;QUOTE: &amp;lt;/h3&amp;gt;&amp;lt;h4&amp;gt; {{ quote }} &amp;lt;/h4&amp;gt;       
   &amp;lt;/div&amp;gt;
&amp;lt;/body&amp;gt;
 
&amp;lt;/html&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;5testing-the-app&quot;&gt;5.Testing the app&lt;/h3&gt;

&lt;p&gt;Before moving ahead, we test the flask app and make sure everything is working fine.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(env) $ python3 app.py
* Serving Flask app &quot;app&quot; (lazy loading)
* Environment: production
  WARNING: This is a development server. Do not use it in a production deployment.
  Use a production WSGI server instead.
* Debug mode: off
* Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The result should be visible if you visit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:5000&lt;/code&gt;. When you are finished press &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl+C&lt;/code&gt; in your terminal window to exit the application server.&lt;/p&gt;

&lt;h2 id=&quot;add-wsgi-to-the-app&quot;&gt;Add WSGI to the app&lt;/h2&gt;

&lt;h3 id=&quot;1-create-wsgi-entry-point&quot;&gt;1. Create WSGI entry point&lt;/h3&gt;

&lt;p&gt;We create a Python file for WSGI that will serve as the entry point for our application. This file defines the behaviour of the Gunicorn server with our application.&lt;/p&gt;

&lt;p&gt;Create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wsgi.py&lt;/code&gt; file in the same directory as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app.py&lt;/code&gt;, import the Flask instance from our application and run it as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from app import app
 
if __name__ == '__main__':
   app.run()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-test-gunicorns-ability&quot;&gt;2. Test Gunicorn’s ability&lt;/h3&gt;

&lt;p&gt;We now test the Gunicorn’s ability to serve the project by binding an address to the WSGI file we just created above.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The WSGI file name is written without &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.py&lt;/code&gt; extension.&lt;/p&gt;

&lt;p&gt;So the syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gunicorn &amp;lt;WSGI_module&amp;gt;:&amp;lt;callable_name&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(env) $ gunicorn --bind 0.0.0.0:5000 wsgi:app
[2020-11-10 17:09:57 +0530] [725] [INFO] Starting gunicorn 20.0.4
[2020-11-10 17:09:57 +0530] [725] [INFO] Listening at: http://0.0.0.0:5000 (725)
[2020-11-10 17:09:57 +0530] [725] [INFO] Using worker: sync
[2020-11-10 17:09:57 +0530] [728] [INFO] Booting worker with pid: 728
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When you visit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:5000&lt;/code&gt;, you should see the same output as seen while running the flask application. However, this time via Gunicorn’s endpoint.&lt;/p&gt;

&lt;p&gt;When you are finished press &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl+C&lt;/code&gt; in your terminal window to exit the application server.&lt;/p&gt;

&lt;h3 id=&quot;3-create-wsgi-socket&quot;&gt;3. Create WSGI socket&lt;/h3&gt;

&lt;p&gt;The communication between Gunicorn and Nginx takes place via a socket. Thus, let’s create a Unix socket file in the same directory as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app.py&lt;/code&gt; as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(env) $ gunicorn --workers 3 --bind unix:/home/movieapp/app.sock -m 777 wsgi:app
[2020-11-10 18:02:36 +0530] [2854] [INFO] Starting gunicorn 20.0.4
[2020-11-10 18:02:36 +0530] [2854] [INFO] Listening at: unix:/home/movieapp/app.sock (2854)
[2020-11-10 18:02:36 +0530] [2854] [INFO] Using worker: sync
[2020-11-10 18:02:36 +0530] [2857] [INFO] Booting worker with pid: 2857
[2020-11-10 18:02:36 +0530] [2858] [INFO] Booting worker with pid: 2858
[2020-11-10 18:02:36 +0530] [2859] [INFO] Booting worker with pid: 2859
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By adding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--workers 3&lt;/code&gt; we tell Gunicorn to start 3 worker processes. We bund it to the WSGI entry point file by providing the path to the project directory. We also set an unmask value while the socket file is being created so that there are no restrictions while accessing it.&lt;/p&gt;

&lt;p&gt;If you notice, an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app.sock&lt;/code&gt; file will be automatically created in the project folder upon execution of the above command.&lt;/p&gt;

&lt;h2 id=&quot;configure-nginx&quot;&gt;Configure Nginx&lt;/h2&gt;

&lt;h3 id=&quot;1-create-an-nginx-configuration-file&quot;&gt;1. Create an Nginx configuration file&lt;/h3&gt;

&lt;p&gt;Gunicorn server is now up and running and it waits for requests to flow in from the socket file in the project directory. Now, we need to configure Nginx to pass web requests to that socket by making some small additions to its configuration file.&lt;/p&gt;

&lt;p&gt;Let the Gunicorn server run, open a new terminal and execute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls /etc/nginx/&lt;/code&gt; command. In the output you would get a list Nginx configuration along with two important directories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sites-available&lt;/code&gt;: contains configuration files for all of your possible applications.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sites-enabled&lt;/code&gt;: contains links to the configuration files that Nginx will actually read and run.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, create a new server block configuration file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/nginx/sites-available/&lt;/code&gt; directory.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(env) $ nano /etc/nginx/sites-available/app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we add the following code into the configuration file and save it.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;server {
   listen 80;
 
   location / {
       include proxy_params;
       proxy_pass http://unix:/home/movieapp/app.sock;
   }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, we open up a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;server&lt;/code&gt; block and define it to listen for requests on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;80&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We also add a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location&lt;/code&gt; block that matches every request. In this block, we include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;proxy_params&lt;/code&gt; file that specifies some general proxy parameters and passes the requests to the socket we defined using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;proxy_pass&lt;/code&gt; directive.&lt;/p&gt;

&lt;h3 id=&quot;2-enable-nginx-server-block&quot;&gt;2. Enable Nginx server block&lt;/h3&gt;

&lt;p&gt;We need to link the file to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sites-enabled&lt;/code&gt; directory to enable the Nginx server block. The syntax for linking the file is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ln -s &amp;lt;source_file&amp;gt; &amp;lt;destination_file&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Actual code will look like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-check-for-errors-in-nginx-configuration-file&quot;&gt;3. Check for errors in Nginx configuration file&lt;/h3&gt;

&lt;p&gt;we can check syntax errors in Nginx configuration file by executing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx -t&lt;/code&gt; command as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(env) $ nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If there are no errors, we restart the Nginx process to read our new config file.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl restart nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4-adjust-firewall&quot;&gt;4. Adjust firewall&lt;/h3&gt;

&lt;p&gt;The final step we need to take care of is adjusting the firewall to allow access to the Nginx server:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(env) $ sudo ufw allow 'Nginx Full'
Skipping adding existing rule
Skipping adding existing rule (v6)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can visit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:80&lt;/code&gt; to view your running flask application.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/flaskapp/1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/faun/deploy-flask-app-with-nginx-using-gunicorn-7fda4f50066a&quot;&gt;Deploy flask app with Nginx using Gunicorn&lt;/a&gt;&lt;/p&gt;</content><author><name>Keval Nagda</name></author><category term="jekyll" /><category term="update" /><summary type="html">In this blog, we learn how to build a movie quote generator flask application with Nginx using Gunicorn. Introduction Gunicorn Gunicorn (Green Unicorn) is a Python Web Server Gateway Interface (WSGI) HTTP server. It is broadly compatible with various web frameworks. Nginx Nginx is an open-source HTTP web server, mail proxy server, and reverse proxy and load balancer for HTTP, TCP, and UDP traffic. Nginx provides high performance and stability with a simple configuration. Why use Gunicorn and Nginx with Flask? Flask is just a web framework and not a web server. Thus to serve a flask application, a web server such as Gunicorn, Nginx or Apache is required to accept HTTP requests. Now, the major advantage of using Nginx and Gunicorn together is that in addition to being a web server, Nginx can also proxy connections to Gunicorn which brings good performance benefits along with the capability to handle a large number of connections with very little CPU usage and memory cost. Build the Flask app 1. Update and install local packages First of all, update your local package index and then install the required packages as follows: # Update the local package index $ sudo apt-get update # Install dependencies $ sudo apt install python3-pip python3-dev python3-venv nginx 2. Create a virtual environment Now, we create a virtual environment env for the Python project using venv module. python3 -m venv env We activate the environment env as follows: source env/bin/activate 3. Install Python dependencies Next, we install Flask and Gunicorn. pip3 install flask gunicorn requests 4. Create the movie quotes app We create a file app.py with the following content in it: import requests from flask import Flask, render_template app = Flask(__name__) @app.route('/') def movieapp(): url = &quot;http://movie-quotes-2.herokuapp.com/api/v1/quotes/random&quot; response = requests.get(url).json() return render_template(&quot;index.html&quot;, film=response['film'], quote=response['content']) if __name__ == '__main__': app.run(debug=False, host='0.0.0.0') Now, create a templates folder with index.html file in it to be used as a template for our flask application. Below is the sample index.html file which can be modified as per your requirements. &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;style type=&quot;text/css&quot;&amp;gt; body { background: black; color: white; } div.container { max-width: 500px; margin: 100px auto; border: 20px solid white; padding: 10px; text-align: center; } h2 { text-transform: uppercase; } &amp;lt;/style&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;div class=&quot;container&quot;&amp;gt; &amp;lt;h2&amp;gt;Quote of the day&amp;lt;/h2&amp;gt; &amp;lt;hr&amp;gt; &amp;lt;h3&amp;gt;FILM: &amp;lt;/h3&amp;gt;&amp;lt;h4&amp;gt; {{ film }} &amp;lt;/h4&amp;gt; &amp;lt;h3&amp;gt;QUOTE: &amp;lt;/h3&amp;gt;&amp;lt;h4&amp;gt; {{ quote }} &amp;lt;/h4&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; 5.Testing the app Before moving ahead, we test the flask app and make sure everything is working fine. (env) $ python3 app.py * Serving Flask app &quot;app&quot; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) The result should be visible if you visit http://localhost:5000. When you are finished press Ctrl+C in your terminal window to exit the application server. Add WSGI to the app 1. Create WSGI entry point We create a Python file for WSGI that will serve as the entry point for our application. This file defines the behaviour of the Gunicorn server with our application. Create wsgi.py file in the same directory as app.py, import the Flask instance from our application and run it as follows: from app import app if __name__ == '__main__': app.run() 2. Test Gunicorn’s ability We now test the Gunicorn’s ability to serve the project by binding an address to the WSGI file we just created above. Note: The WSGI file name is written without .py extension. So the syntax is gunicorn &amp;lt;WSGI_module&amp;gt;:&amp;lt;callable_name&amp;gt;. (env) $ gunicorn --bind 0.0.0.0:5000 wsgi:app [2020-11-10 17:09:57 +0530] [725] [INFO] Starting gunicorn 20.0.4 [2020-11-10 17:09:57 +0530] [725] [INFO] Listening at: http://0.0.0.0:5000 (725) [2020-11-10 17:09:57 +0530] [725] [INFO] Using worker: sync [2020-11-10 17:09:57 +0530] [728] [INFO] Booting worker with pid: 728 When you visit http://localhost:5000, you should see the same output as seen while running the flask application. However, this time via Gunicorn’s endpoint. When you are finished press Ctrl+C in your terminal window to exit the application server. 3. Create WSGI socket The communication between Gunicorn and Nginx takes place via a socket. Thus, let’s create a Unix socket file in the same directory as app.py as follows: (env) $ gunicorn --workers 3 --bind unix:/home/movieapp/app.sock -m 777 wsgi:app [2020-11-10 18:02:36 +0530] [2854] [INFO] Starting gunicorn 20.0.4 [2020-11-10 18:02:36 +0530] [2854] [INFO] Listening at: unix:/home/movieapp/app.sock (2854) [2020-11-10 18:02:36 +0530] [2854] [INFO] Using worker: sync [2020-11-10 18:02:36 +0530] [2857] [INFO] Booting worker with pid: 2857 [2020-11-10 18:02:36 +0530] [2858] [INFO] Booting worker with pid: 2858 [2020-11-10 18:02:36 +0530] [2859] [INFO] Booting worker with pid: 2859 By adding --workers 3 we tell Gunicorn to start 3 worker processes. We bund it to the WSGI entry point file by providing the path to the project directory. We also set an unmask value while the socket file is being created so that there are no restrictions while accessing it. If you notice, an app.sock file will be automatically created in the project folder upon execution of the above command. Configure Nginx 1. Create an Nginx configuration file Gunicorn server is now up and running and it waits for requests to flow in from the socket file in the project directory. Now, we need to configure Nginx to pass web requests to that socket by making some small additions to its configuration file. Let the Gunicorn server run, open a new terminal and execute ls /etc/nginx/ command. In the output you would get a list Nginx configuration along with two important directories: sites-available: contains configuration files for all of your possible applications. sites-enabled: contains links to the configuration files that Nginx will actually read and run. Now, create a new server block configuration file app in /etc/nginx/sites-available/ directory. (env) $ nano /etc/nginx/sites-available/app Next, we add the following code into the configuration file and save it. server { listen 80; location / { include proxy_params; proxy_pass http://unix:/home/movieapp/app.sock; } } Here, we open up a server block and define it to listen for requests on port 80. We also add a location block that matches every request. In this block, we include proxy_params file that specifies some general proxy parameters and passes the requests to the socket we defined using the proxy_pass directive. 2. Enable Nginx server block We need to link the file to the sites-enabled directory to enable the Nginx server block. The syntax for linking the file is as follows: ln -s &amp;lt;source_file&amp;gt; &amp;lt;destination_file&amp;gt; Actual code will look like: ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/ 3. Check for errors in Nginx configuration file we can check syntax errors in Nginx configuration file by executing the nginx -t command as follows: (env) $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful If there are no errors, we restart the Nginx process to read our new config file. systemctl restart nginx 4. Adjust firewall The final step we need to take care of is adjusting the firewall to allow access to the Nginx server: (env) $ sudo ufw allow 'Nginx Full' Skipping adding existing rule Skipping adding existing rule (v6) You can visit http://localhost:80 to view your running flask application.</summary></entry><entry><title type="html">Set up Jenkins in Docker</title><link href="http://localhost:4000/jenkins-in-docker" rel="alternate" type="text/html" title="Set up Jenkins in Docker" /><published>2020-11-10T12:00:00+05:30</published><updated>2020-11-10T12:00:00+05:30</updated><id>http://localhost:4000/jenkins-in-docker</id><content type="html" xml:base="http://localhost:4000/jenkins-in-docker">&lt;p&gt;In this blog, we learn how to run Jenkins in a Docker container.&lt;/p&gt;

&lt;h2 id=&quot;what-is-jenkins&quot;&gt;What is Jenkins?&lt;/h2&gt;

&lt;p&gt;Jenkins is a self-contained, open-source automation server that can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software.&lt;/p&gt;

&lt;p&gt;Jenkins can be installed through native system packages, Docker, or even run standalone by any machine with a Java Runtime Environment (JRE) installed.&lt;/p&gt;

&lt;h2 id=&quot;step-1-pull-jenkins-docker-image&quot;&gt;Step 1: Pull Jenkins Docker image&lt;/h2&gt;

&lt;p&gt;We pull the [Jenkins Docker image] from the Docker Hub repository.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull jenkins/jenkins
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;step-2-run-a-docker-container&quot;&gt;Step 2: Run a Docker container&lt;/h2&gt;

&lt;p&gt;We can run a docker container based on the Docker image we just pulled.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -p 8080:8080 -p 50000:50000 jenkins/jenkins:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where we expose port 8080 on the host and docker server to access Jenkins dashboard and port 50000 for the Jenkins API.&lt;/p&gt;

&lt;p&gt;This command will create and run a Docker container for you, however, it won’t save any data created when the container is exited or shutdown.&lt;/p&gt;

&lt;p&gt;We can have persistent storage for Jenkins by executing the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -p 8080:8080 -p 50000:50000 -v /home/projects/Jenkins_Home:/var/jenkins_home jenkins/jenkins:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/projects/Jenkins_Home&lt;/code&gt; can be replaced by the path where you wish to store your Jenkins data.&lt;/p&gt;

&lt;p&gt;However, I would recommend using a Docker volume to avoid permission issues while accessing the directory and let Docker handle the storage functionality.&lt;/p&gt;

&lt;p&gt;To use a Docker volume for Jenkins data, simply just create a volume as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker volume create myjenkins
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, to use this volume to store Jenkins data execute the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -p 8080:8080 -p 50000:50000 -v myjenkins:/var/jenkins_home jenkins/jenkins:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;step-3-set-up-jenkins&quot;&gt;Step 3: Set up Jenkins&lt;/h2&gt;

&lt;p&gt;Once Jenkins files have been extracted, the Jenkins server will be fully up and running at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:8080&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can find the initial admin password at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/jenkins_home/secrets/initialAdminPassword&lt;/code&gt; as mentioned on the login page.&lt;/p&gt;

&lt;p&gt;Next, we can install plugins as per our requirement.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It may take some time depending upon the number of plugins you choose to install. Once the plugins are installed, you will be prompted to create a first admin user which you can skip if you wish to continue as an admin user.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/jenkins/4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Congratulations! You just set up the Jenkins server inside a Docker container.&lt;/p&gt;</content><author><name>Keval Nagda</name></author><category term="jekyll" /><category term="update" /><summary type="html">In this blog, we learn how to run Jenkins in a Docker container. What is Jenkins? Jenkins is a self-contained, open-source automation server that can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software. Jenkins can be installed through native system packages, Docker, or even run standalone by any machine with a Java Runtime Environment (JRE) installed. Step 1: Pull Jenkins Docker image We pull the [Jenkins Docker image] from the Docker Hub repository. docker pull jenkins/jenkins Step 2: Run a Docker container We can run a docker container based on the Docker image we just pulled. docker run -p 8080:8080 -p 50000:50000 jenkins/jenkins:latest where we expose port 8080 on the host and docker server to access Jenkins dashboard and port 50000 for the Jenkins API. This command will create and run a Docker container for you, however, it won’t save any data created when the container is exited or shutdown. We can have persistent storage for Jenkins by executing the following command: docker run -p 8080:8080 -p 50000:50000 -v /home/projects/Jenkins_Home:/var/jenkins_home jenkins/jenkins:latest Here, /home/projects/Jenkins_Home can be replaced by the path where you wish to store your Jenkins data. However, I would recommend using a Docker volume to avoid permission issues while accessing the directory and let Docker handle the storage functionality. To use a Docker volume for Jenkins data, simply just create a volume as follows: docker volume create myjenkins Now, to use this volume to store Jenkins data execute the following command: docker run -p 8080:8080 -p 50000:50000 -v myjenkins:/var/jenkins_home jenkins/jenkins:latest Step 3: Set up Jenkins Once Jenkins files have been extracted, the Jenkins server will be fully up and running at http://localhost:8080. You can find the initial admin password at /var/jenkins_home/secrets/initialAdminPassword as mentioned on the login page. Next, we can install plugins as per our requirement. It may take some time depending upon the number of plugins you choose to install. Once the plugins are installed, you will be prompted to create a first admin user which you can skip if you wish to continue as an admin user. Congratulations! You just set up the Jenkins server inside a Docker container.</summary></entry><entry><title type="html">Conda Docker tutorial</title><link href="http://localhost:4000/conda-docker-tutorial" rel="alternate" type="text/html" title="Conda Docker tutorial" /><published>2020-11-10T12:00:00+05:30</published><updated>2020-11-10T12:00:00+05:30</updated><id>http://localhost:4000/conda-docker-tutorial</id><content type="html" xml:base="http://localhost:4000/conda-docker-tutorial">&lt;p&gt;Conda is an open-source package manager that helps you quickly install, run and update packages and their dependencies. It helps you easily create, save, load and switch between different environments on your system.
Dockerfile is a text file that defines a set of commands or operations which aid you to build your own custom Docker image. And in order to build a Conda-based application, you’ll need to activate a Conda environment in the Dockerfile.&lt;/p&gt;

&lt;p&gt;Unfortunately, the approach of activating Conda environments in a Dockerfile is a bit different than you would expect.&lt;/p&gt;

&lt;h2 id=&quot;defining-the-conda-environment&quot;&gt;Defining the Conda environment&lt;/h2&gt;
&lt;p&gt;Firstly, let’s create an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt; file to define the Conda environment.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;name: env
channels:
   - conda-forge
dependencies:
   - python=3.6
   - flask
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;env&lt;/code&gt; is the name of our Conda environment. We use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda-forge&lt;/code&gt; channel to utilize Conda package provided by the conda-forge community and install two dependencies: python and flask.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt; is similar to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; used by virtual environment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;venv&lt;/code&gt; module.&lt;/p&gt;

&lt;h2 id=&quot;a-simple-python-program&quot;&gt;A simple Python program&lt;/h2&gt;

&lt;p&gt;Write a simple Python program to test the Conda environment activation.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import flask
 
print(&quot;Flask import successful!&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;first-attempt&quot;&gt;First attempt&lt;/h2&gt;

&lt;p&gt;Following the standard approach, our first iteration of the Dockerfile looks like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Define base image
FROM continuumio/miniconda3
 
# Set working directory for the project
WORKDIR /app
 
# Create Conda environment from the YAML file
COPY environment.yml .
RUN conda env create -f environment.yml
 
# Activate Conda environment and check if it is working properly
RUN conda activate env
RUN echo &quot;Making sure flask is installed correctly...&quot;
RUN python -c &quot;import flask&quot;
 
# Python program to run in the container
COPY run.py .
ENTRYPOINT [&quot;python&quot;, &quot;run.py&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Following is the result when we try building the Docker image&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Step 5/9 : RUN conda activate env
---&amp;gt; Running in e33a2dcd4d99
 
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run
 
   $ conda init &amp;lt;SHELL_NAME&amp;gt;
 
The command '/bin/sh -c conda activate env' returned a non-zero code: 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We notice that we can’t just emulate the Conda environment and we need to use Conda’s own activation method.&lt;/p&gt;

&lt;h2 id=&quot;second-attempt&quot;&gt;Second attempt&lt;/h2&gt;

&lt;p&gt;We can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda init bash&lt;/code&gt; as a solution to the above problem. Docker by default uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/bin/sh -c &amp;lt;command&amp;gt;&lt;/code&gt; to execute instructions but we require a bash shell to run the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda init bash&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;To tackle this, we override the default shell by adding the following to the Dockerfile:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SHELL [&quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our updated Dockerfile should look as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Define base image
FROM continuumio/miniconda3
 
# Set working directory for the project
WORKDIR /app
 
# Override default shell and use bash
SHELL [&quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot;]
 
# Create Conda environment from the YAML file
COPY environment.yml .
RUN conda env create -f environment.yml
 
# Initialize conda in the bash config files
RUN conda init bash
 
# Activate Conda environment and check if it is working properly
RUN conda activate env
RUN echo &quot;Making sure flask is installed correctly...&quot;
RUN python -c &quot;import flask&quot;
 
# Python program to run in the container
COPY run.py .
ENTRYPOINT [&quot;python&quot;, &quot;run.py&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Building the Docker image again gives us the following result:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Step 9/11 : RUN python -c &quot;import flask&quot;
---&amp;gt; Running in ea831eb42ff6
Traceback (most recent call last):
 File &quot;&amp;lt;string&amp;gt;&quot;, line 1, in &amp;lt;module&amp;gt;
ModuleNotFoundError: No module named 'flask'
The command '/bin/bash --login -c python -c &quot;import flask&quot;' returned a non-zero code: 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So the problem is that each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RUN&lt;/code&gt; instruction in a Dockerfile executes in a separate run of bash. Thus, in the above example Conda environment is activated in the first &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RUN&lt;/code&gt; and later &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RUN&lt;/code&gt;s are new shell sessions without Conda activation.&lt;/p&gt;

&lt;h2 id=&quot;third-attempt&quot;&gt;Third attempt&lt;/h2&gt;

&lt;p&gt;Now, since each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RUN&lt;/code&gt; instruction is a separate run of bash, adding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda activate&lt;/code&gt; command to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt; of the current user should work. It will work perfectly fine and you will be able to build a Docker image, however, when you run a container based on that image, it will result in the same error as above.&lt;/p&gt;

&lt;p&gt;This is due to the fact that we are using &lt;em&gt;exec&lt;/em&gt; form of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENTRYPOINT&lt;/code&gt; instruction which doesn’t actually start a shell session. An alternative to this is the &lt;em&gt;shell&lt;/em&gt; form of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENTRYPOINT&lt;/code&gt; i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENTRYPOINT python run.py&lt;/code&gt; but this won’t work since it breaks down the container.&lt;/p&gt;

&lt;p&gt;The final resort to this problem is by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda run -n env &amp;lt;command&amp;gt;&lt;/code&gt; instruction which actually runs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;command&amp;gt;&lt;/code&gt; inside the conda environment.&lt;/p&gt;

&lt;p&gt;So the final working Dockerfile should look like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Define base image
FROM continuumio/miniconda3
 
# Set working directory for the project
WORKDIR /app
 
# Create Conda environment from the YAML file
COPY environment.yml .
RUN conda env create -f environment.yml
 
# Override default shell and use bash
SHELL [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;env&quot;, &quot;/bin/bash&quot;, &quot;-c&quot;]
 
# Activate Conda environment and check if it is working properly
RUN echo &quot;Making sure flask is installed correctly...&quot;
RUN python -c &quot;import flask&quot;
 
# Python program to run in the container
COPY run.py .
ENTRYPOINT [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;env&quot;, &quot;python&quot;, &quot;run.py&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the result obtained is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ docker build .
Sending build context to Docker daemon  4.608kB
Step 1/9 : FROM continuumio/miniconda3
---&amp;gt; b4adc22212f1
Step 2/9 : WORKDIR /app
---&amp;gt; Using cache
---&amp;gt; 658c526932d9
Step 3/9 : COPY environment.yml .
---&amp;gt; 26ff11b33587
Step 4/9 : RUN conda env create -f environment.yml
---&amp;gt; Running in d6729c106f2b
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done
 
Downloading and Extracting Packages
_libgcc_mutex-0.1    | 3 KB      | ########## | 100%
flask-1.1.2          | 70 KB     | ########## | 100%
zlib-1.2.11          | 106 KB    | ########## | 100%
wheel-0.35.1         | 29 KB     | ########## | 100%
ncurses-6.2          | 1022 KB   | ########## | 100%
werkzeug-1.0.1       | 239 KB    | ########## | 100%
ld_impl_linux-64-2.3 | 617 KB    | ########## | 100%
click-7.1.2          | 64 KB     | ########## | 100%
markupsafe-1.1.1     | 27 KB     | ########## | 100%
libffi-3.2.1         | 47 KB     | ########## | 100%
openssl-1.1.1h       | 2.1 MB    | ########## | 100%
python-3.6.11        | 34.2 MB   | ########## | 100%
itsdangerous-1.1.0   | 16 KB     | ########## | 100%
libstdcxx-ng-9.3.0   | 4.0 MB    | ########## | 100%
xz-5.2.5             | 343 KB    | ########## | 100%
libgcc-ng-9.3.0      | 7.8 MB    | ########## | 100%
setuptools-49.6.0    | 947 KB    | ########## | 100%
jinja2-2.11.2        | 93 KB     | ########## | 100%
ca-certificates-2020 | 145 KB    | ########## | 100%
certifi-2020.6.20    | 151 KB    | ########## | 100%
pip-20.2.4           | 1.1 MB    | ########## | 100%
libgomp-9.3.0        | 378 KB    | ########## | 100%
tk-8.6.10            | 3.2 MB    | ########## | 100%
python_abi-3.6       | 4 KB      | ########## | 100%
_openmp_mutex-4.5    | 22 KB     | ########## | 100%
sqlite-3.33.0        | 1.4 MB    | ########## | 100%
readline-8.0         | 281 KB    | ########## | 100%
Preparing transaction: ...working... done
Verifying transaction: ...working... done
Executing transaction: ...working... done
#
# To activate this environment, use
#
#     $ conda activate env
#
# To deactivate an active environment, use
#
#     $ conda deactivate
 
==&amp;gt; WARNING: A newer version of conda exists. &amp;lt;==
 current version: 4.8.2
 latest version: 4.9.1
 
Please update conda by running
 
   $ conda update -n base -c defaults conda
 
Removing intermediate container d6729c106f2b
---&amp;gt; c03cbd024136
Step 5/9 : SHELL [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;env&quot;, &quot;/bin/bash&quot;, &quot;-c&quot;]
---&amp;gt; Running in f37ed240c11e
Removing intermediate container f37ed240c11e
---&amp;gt; b56ab8574a14
Step 6/9 : RUN echo &quot;Making sure flask is installed correctly...&quot;
---&amp;gt; Running in 46b28807a8e2
Making sure flask is installed correctly...
 
Removing intermediate container 46b28807a8e2
---&amp;gt; 70228fcf11ec
Step 7/9 : RUN python -c &quot;import flask&quot;
---&amp;gt; Running in b5a021d87998
Removing intermediate container b5a021d87998
---&amp;gt; 846df181bd85
Step 8/9 : COPY run.py .
---&amp;gt; 975a983a3504
Step 9/9 : ENTRYPOINT [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;env&quot;, &quot;python&quot;, &quot;run.py&quot;]
---&amp;gt; Running in 240d3476910c
Removing intermediate container 240d3476910c
---&amp;gt; 72a352583f00
Successfully built 72a352583f00
 
$ docker images
REPOSITORY               TAG                 IMAGE ID            CREATED             SIZE
&amp;lt;none&amp;gt;                   &amp;lt;none&amp;gt;              72a352583f00        2 minutes ago       964MB
 
$ docker run 72a352583f00
Flask import successful!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://pythonspeed.com/articles/activate-conda-dockerfile/&quot;&gt;Activating a Conda environment in your Dockerfile&lt;/a&gt;&lt;/p&gt;</content><author><name>Keval Nagda</name></author><category term="jekyll" /><category term="update" /><summary type="html">Conda is an open-source package manager that helps you quickly install, run and update packages and their dependencies. It helps you easily create, save, load and switch between different environments on your system. Dockerfile is a text file that defines a set of commands or operations which aid you to build your own custom Docker image. And in order to build a Conda-based application, you’ll need to activate a Conda environment in the Dockerfile. Unfortunately, the approach of activating Conda environments in a Dockerfile is a bit different than you would expect. Defining the Conda environment Firstly, let’s create an environment.yml file to define the Conda environment. name: env channels: - conda-forge dependencies: - python=3.6 - flask Here env is the name of our Conda environment. We use conda-forge channel to utilize Conda package provided by the conda-forge community and install two dependencies: python and flask. environment.yml is similar to the requirements.txt used by virtual environment venv module. A simple Python program Write a simple Python program to test the Conda environment activation. import flask print(&quot;Flask import successful!&quot;) First attempt Following the standard approach, our first iteration of the Dockerfile looks like: # Define base image FROM continuumio/miniconda3 # Set working directory for the project WORKDIR /app # Create Conda environment from the YAML file COPY environment.yml . RUN conda env create -f environment.yml # Activate Conda environment and check if it is working properly RUN conda activate env RUN echo &quot;Making sure flask is installed correctly...&quot; RUN python -c &quot;import flask&quot; # Python program to run in the container COPY run.py . ENTRYPOINT [&quot;python&quot;, &quot;run.py&quot;] Following is the result when we try building the Docker image Step 5/9 : RUN conda activate env ---&amp;gt; Running in e33a2dcd4d99 CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'. To initialize your shell, run $ conda init &amp;lt;SHELL_NAME&amp;gt; The command '/bin/sh -c conda activate env' returned a non-zero code: 1 We notice that we can’t just emulate the Conda environment and we need to use Conda’s own activation method. Second attempt We can use conda init bash as a solution to the above problem. Docker by default uses /bin/sh -c &amp;lt;command&amp;gt; to execute instructions but we require a bash shell to run the conda init bash command. To tackle this, we override the default shell by adding the following to the Dockerfile: SHELL [&quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot;] Our updated Dockerfile should look as follows: # Define base image FROM continuumio/miniconda3 # Set working directory for the project WORKDIR /app # Override default shell and use bash SHELL [&quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot;] # Create Conda environment from the YAML file COPY environment.yml . RUN conda env create -f environment.yml # Initialize conda in the bash config files RUN conda init bash # Activate Conda environment and check if it is working properly RUN conda activate env RUN echo &quot;Making sure flask is installed correctly...&quot; RUN python -c &quot;import flask&quot; # Python program to run in the container COPY run.py . ENTRYPOINT [&quot;python&quot;, &quot;run.py&quot;] Building the Docker image again gives us the following result: Step 9/11 : RUN python -c &quot;import flask&quot; ---&amp;gt; Running in ea831eb42ff6 Traceback (most recent call last): File &quot;&amp;lt;string&amp;gt;&quot;, line 1, in &amp;lt;module&amp;gt; ModuleNotFoundError: No module named 'flask' The command '/bin/bash --login -c python -c &quot;import flask&quot;' returned a non-zero code: 1 So the problem is that each RUN instruction in a Dockerfile executes in a separate run of bash. Thus, in the above example Conda environment is activated in the first RUN and later RUNs are new shell sessions without Conda activation. Third attempt Now, since each RUN instruction is a separate run of bash, adding conda activate command to the ~/.bashrc of the current user should work. It will work perfectly fine and you will be able to build a Docker image, however, when you run a container based on that image, it will result in the same error as above. This is due to the fact that we are using exec form of ENTRYPOINT instruction which doesn’t actually start a shell session. An alternative to this is the shell form of ENTRYPOINT i.e. ENTRYPOINT python run.py but this won’t work since it breaks down the container. The final resort to this problem is by using conda run -n env &amp;lt;command&amp;gt; instruction which actually runs &amp;lt;command&amp;gt; inside the conda environment. So the final working Dockerfile should look like: # Define base image FROM continuumio/miniconda3 # Set working directory for the project WORKDIR /app # Create Conda environment from the YAML file COPY environment.yml . RUN conda env create -f environment.yml # Override default shell and use bash SHELL [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;env&quot;, &quot;/bin/bash&quot;, &quot;-c&quot;] # Activate Conda environment and check if it is working properly RUN echo &quot;Making sure flask is installed correctly...&quot; RUN python -c &quot;import flask&quot; # Python program to run in the container COPY run.py . ENTRYPOINT [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;env&quot;, &quot;python&quot;, &quot;run.py&quot;] And the result obtained is as follows: $ docker build . Sending build context to Docker daemon 4.608kB Step 1/9 : FROM continuumio/miniconda3 ---&amp;gt; b4adc22212f1 Step 2/9 : WORKDIR /app ---&amp;gt; Using cache ---&amp;gt; 658c526932d9 Step 3/9 : COPY environment.yml . ---&amp;gt; 26ff11b33587 Step 4/9 : RUN conda env create -f environment.yml ---&amp;gt; Running in d6729c106f2b Collecting package metadata (repodata.json): ...working... done Solving environment: ...working... done Downloading and Extracting Packages _libgcc_mutex-0.1 | 3 KB | ########## | 100% flask-1.1.2 | 70 KB | ########## | 100% zlib-1.2.11 | 106 KB | ########## | 100% wheel-0.35.1 | 29 KB | ########## | 100% ncurses-6.2 | 1022 KB | ########## | 100% werkzeug-1.0.1 | 239 KB | ########## | 100% ld_impl_linux-64-2.3 | 617 KB | ########## | 100% click-7.1.2 | 64 KB | ########## | 100% markupsafe-1.1.1 | 27 KB | ########## | 100% libffi-3.2.1 | 47 KB | ########## | 100% openssl-1.1.1h | 2.1 MB | ########## | 100% python-3.6.11 | 34.2 MB | ########## | 100% itsdangerous-1.1.0 | 16 KB | ########## | 100% libstdcxx-ng-9.3.0 | 4.0 MB | ########## | 100% xz-5.2.5 | 343 KB | ########## | 100% libgcc-ng-9.3.0 | 7.8 MB | ########## | 100% setuptools-49.6.0 | 947 KB | ########## | 100% jinja2-2.11.2 | 93 KB | ########## | 100% ca-certificates-2020 | 145 KB | ########## | 100% certifi-2020.6.20 | 151 KB | ########## | 100% pip-20.2.4 | 1.1 MB | ########## | 100% libgomp-9.3.0 | 378 KB | ########## | 100% tk-8.6.10 | 3.2 MB | ########## | 100% python_abi-3.6 | 4 KB | ########## | 100% _openmp_mutex-4.5 | 22 KB | ########## | 100% sqlite-3.33.0 | 1.4 MB | ########## | 100% readline-8.0 | 281 KB | ########## | 100% Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done # # To activate this environment, use # # $ conda activate env # # To deactivate an active environment, use # # $ conda deactivate ==&amp;gt; WARNING: A newer version of conda exists. &amp;lt;== current version: 4.8.2 latest version: 4.9.1 Please update conda by running $ conda update -n base -c defaults conda Removing intermediate container d6729c106f2b ---&amp;gt; c03cbd024136 Step 5/9 : SHELL [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;env&quot;, &quot;/bin/bash&quot;, &quot;-c&quot;] ---&amp;gt; Running in f37ed240c11e Removing intermediate container f37ed240c11e ---&amp;gt; b56ab8574a14 Step 6/9 : RUN echo &quot;Making sure flask is installed correctly...&quot; ---&amp;gt; Running in 46b28807a8e2 Making sure flask is installed correctly... Removing intermediate container 46b28807a8e2 ---&amp;gt; 70228fcf11ec Step 7/9 : RUN python -c &quot;import flask&quot; ---&amp;gt; Running in b5a021d87998 Removing intermediate container b5a021d87998 ---&amp;gt; 846df181bd85 Step 8/9 : COPY run.py . ---&amp;gt; 975a983a3504 Step 9/9 : ENTRYPOINT [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;env&quot;, &quot;python&quot;, &quot;run.py&quot;] ---&amp;gt; Running in 240d3476910c Removing intermediate container 240d3476910c ---&amp;gt; 72a352583f00 Successfully built 72a352583f00 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE &amp;lt;none&amp;gt; &amp;lt;none&amp;gt; 72a352583f00 2 minutes ago 964MB $ docker run 72a352583f00 Flask import successful! References</summary></entry><entry><title type="html">Dockerfile best practices</title><link href="http://localhost:4000/dockerfile-best-practices" rel="alternate" type="text/html" title="Dockerfile best practices" /><published>2020-11-09T13:30:08+05:30</published><updated>2020-11-09T13:30:08+05:30</updated><id>http://localhost:4000/dockerfile-best-practices</id><content type="html" xml:base="http://localhost:4000/dockerfile-best-practices">&lt;p&gt;Learning Docker and building Docker images from Dockerfile can be daunting at times, especially when you are a beginner. Following are a few important points to remember while dealing with Dockerfile and Docker images.&lt;/p&gt;

&lt;h2 id=&quot;minimize-the-number-of-steps-in-the-dockerfile&quot;&gt;Minimize the number of steps in the Dockerfile&lt;/h2&gt;

&lt;p&gt;Minimizing the number of steps in the Dockerfile not only helps you to improve the build but also significantly improves the pull performance.
Also, combining several steps into one line tends to create a single intermediary image instead of several i.e. each for one step.&lt;/p&gt;

&lt;h2 id=&quot;start-your-dockerfile-with-the-steps-that-are-least-likely-to-change&quot;&gt;Start your Dockerfile with the steps that are least likely to change&lt;/h2&gt;

&lt;p&gt;This is the best advice one can get while learning to build a Docker image from Dockerfile. Usually, the best practice is to structure your Dockerfile as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Install the tools needed to build your application.&lt;/li&gt;
  &lt;li&gt;Install all the required dependencies, libraries and packages.&lt;/li&gt;
  &lt;li&gt;Finally, build your application.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;a-fairly-straightforward-approach-to-building-dockerfiles-in-an-iterative-manner-would-be-as-follows&quot;&gt;A fairly straightforward approach to building Dockerfiles in an iterative manner would be as follows:&lt;/h2&gt;

&lt;h3 id=&quot;1-pick-the-right-base-image&quot;&gt;1. Pick the right base image&lt;/h3&gt;

&lt;p&gt;Picking the right image can be confusing at times. Thus, you should experiment with the one that best suits your requirements. For example to build a simple python application, one can select &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python&lt;/code&gt; as their base image instead of selecting a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu&lt;/code&gt; base image and installing dependencies on top of it.&lt;/p&gt;

&lt;h3 id=&quot;2-go-to-the-shell-and-build-your-environment&quot;&gt;2. Go to the shell and build your environment&lt;/h3&gt;

&lt;p&gt;Building a docker image every time you make changes to your Dockerfile can be a hectic and time-consuming process. An alternative and efficient solution to this is to pull the preferred image locally and start a container in an interactive shell mode.&lt;/p&gt;

&lt;p&gt;Once the steps execute in the container perfectly as needed, you can add those instructions to the Dockerfile immediately.&lt;/p&gt;

&lt;h3 id=&quot;3-add-the-steps-to-your-dockerfile-and-build-your-image&quot;&gt;3. Add the steps to your Dockerfile and build your image&lt;/h3&gt;

&lt;p&gt;Stopping in middle, building and testing the docker image from the Dockerfile is also a crucial step. This step makes sure that you get the same desired results every time.&lt;/p&gt;

&lt;p&gt;This newly built image can then be used to instantiate a new container with an interactive shell mode to proceed with installation and set-up steps.&lt;/p&gt;

&lt;h3 id=&quot;4-repeat-steps-2-and-3&quot;&gt;4. Repeat steps 2 and 3&lt;/h3&gt;

&lt;p&gt;You might need to repeat steps 2 and 3 several times in order to thoroughly build a failproof Docker image and make sure that everything works fine as expected.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://docs.docker.com/engine/reference/builder/&quot;&gt;Dockerfile reference&lt;/a&gt; is a good place to look for common Dockerfile syntax, warnings and documentation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://takacsmark.com/dockerfile-tutorial-by-example-dockerfile-best-practices-2018/&quot;&gt;Dockerfile tutorial by example&lt;/a&gt;&lt;/p&gt;</content><author><name>Keval Nagda</name></author><category term="jekyll" /><category term="update" /><summary type="html">Learning Docker and building Docker images from Dockerfile can be daunting at times, especially when you are a beginner. Following are a few important points to remember while dealing with Dockerfile and Docker images. Minimize the number of steps in the Dockerfile Minimizing the number of steps in the Dockerfile not only helps you to improve the build but also significantly improves the pull performance. Also, combining several steps into one line tends to create a single intermediary image instead of several i.e. each for one step. Start your Dockerfile with the steps that are least likely to change This is the best advice one can get while learning to build a Docker image from Dockerfile. Usually, the best practice is to structure your Dockerfile as follows: Install the tools needed to build your application. Install all the required dependencies, libraries and packages. Finally, build your application. A fairly straightforward approach to building Dockerfiles in an iterative manner would be as follows: 1. Pick the right base image Picking the right image can be confusing at times. Thus, you should experiment with the one that best suits your requirements. For example to build a simple python application, one can select python as their base image instead of selecting a ubuntu base image and installing dependencies on top of it. 2. Go to the shell and build your environment Building a docker image every time you make changes to your Dockerfile can be a hectic and time-consuming process. An alternative and efficient solution to this is to pull the preferred image locally and start a container in an interactive shell mode. Once the steps execute in the container perfectly as needed, you can add those instructions to the Dockerfile immediately. 3. Add the steps to your Dockerfile and build your image Stopping in middle, building and testing the docker image from the Dockerfile is also a crucial step. This step makes sure that you get the same desired results every time. This newly built image can then be used to instantiate a new container with an interactive shell mode to proceed with installation and set-up steps. 4. Repeat steps 2 and 3 You might need to repeat steps 2 and 3 several times in order to thoroughly build a failproof Docker image and make sure that everything works fine as expected. Dockerfile reference is a good place to look for common Dockerfile syntax, warnings and documentation. References Dockerfile tutorial by example</summary></entry></feed>